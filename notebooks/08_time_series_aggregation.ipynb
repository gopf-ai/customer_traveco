{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Aggregation (2022-2025)\n",
    "\n",
    "This notebook creates monthly aggregated time series data by combining:\n",
    "1. Historic data (2022-2024) from notebook 07\n",
    "\n",
    "## Objectives\n",
    "1. Load consolidated historic data (36 months)\n",
    "2. Load June 2025 processed data\n",
    "3. Create monthly aggregations by Betriebszentralen\n",
    "4. Merge into complete time series (42 months total)\n",
    "5. Prepare data for forecasting models\n",
    "\n",
    "## Output\n",
    "- **Time range**: January 2022 - December 2024 (36 months)\n",
    "- **Granularity**: Monthly aggregations by Betriebszentralen (14 dispatch centers)\n",
    "- **Metrics**: Orders, KM, carriers, costs, efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Historic Data (2022-2024)\n",
    "\n",
    "Load the consolidated historic dataset created in notebook 07."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historic data from ../data/processed/historic_orders_2022_2024.parquet...\n",
      "✓ Loaded 4,937,096 records\n",
      "\n",
      "Dataset info:\n",
      "  Shape: (4937096, 121)\n",
      "  Date range: 2022-01-01 00:00:00 to 2024-12-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x11157da10>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kk/.local/share/virtualenvs/dev-_bs4bCos/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 781, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "  File \"/Users/kk/.local/share/virtualenvs/dev-_bs4bCos/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 600, in sigint_handler\n",
      "    raise KeyboardInterrupt\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_historic.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Date range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_historic[\u001b[33m'\u001b[39m\u001b[33mDatum.Tour\u001b[39m\u001b[33m'\u001b[39m].min()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_historic[\u001b[33m'\u001b[39m\u001b[33mDatum.Tour\u001b[39m\u001b[33m'\u001b[39m].max()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Memory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf_historic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmemory_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m.sum()\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m1024\u001b[39m**\u001b[32m3\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m GB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/dev-_bs4bCos/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[39m, in \u001b[36mDataFrame.memory_usage\u001b[39m\u001b[34m(self, index, deep)\u001b[39m\n\u001b[32m   3671\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmemory_usage\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m, deep: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> Series:\n\u001b[32m   3672\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3673\u001b[39m \u001b[33;03m    Return the memory usage of each column in bytes.\u001b[39;00m\n\u001b[32m   3674\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3758\u001b[39m \u001b[33;03m    5244\u001b[39;00m\n\u001b[32m   3759\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   3760\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._constructor_sliced(\n\u001b[32m-> \u001b[39m\u001b[32m3761\u001b[39m         \u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmemory_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[32m   3762\u001b[39m         index=\u001b[38;5;28mself\u001b[39m.columns,\n\u001b[32m   3763\u001b[39m         dtype=np.intp,\n\u001b[32m   3764\u001b[39m     )\n\u001b[32m   3765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index:\n\u001b[32m   3766\u001b[39m         index_memory_usage = \u001b[38;5;28mself\u001b[39m._constructor_sliced(\n\u001b[32m   3767\u001b[39m             \u001b[38;5;28mself\u001b[39m.index.memory_usage(deep=deep), index=[\u001b[33m\"\u001b[39m\u001b[33mIndex\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3768\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/dev-_bs4bCos/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   3671\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmemory_usage\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m, deep: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> Series:\n\u001b[32m   3672\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3673\u001b[39m \u001b[33;03m    Return the memory usage of each column in bytes.\u001b[39;00m\n\u001b[32m   3674\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3758\u001b[39m \u001b[33;03m    5244\u001b[39;00m\n\u001b[32m   3759\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   3760\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._constructor_sliced(\n\u001b[32m-> \u001b[39m\u001b[32m3761\u001b[39m         [\u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmemory_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m col, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()],\n\u001b[32m   3762\u001b[39m         index=\u001b[38;5;28mself\u001b[39m.columns,\n\u001b[32m   3763\u001b[39m         dtype=np.intp,\n\u001b[32m   3764\u001b[39m     )\n\u001b[32m   3765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index:\n\u001b[32m   3766\u001b[39m         index_memory_usage = \u001b[38;5;28mself\u001b[39m._constructor_sliced(\n\u001b[32m   3767\u001b[39m             \u001b[38;5;28mself\u001b[39m.index.memory_usage(deep=deep), index=[\u001b[33m\"\u001b[39m\u001b[33mIndex\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3768\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/dev-_bs4bCos/lib/python3.11/site-packages/pandas/core/series.py:5492\u001b[39m, in \u001b[36mSeries.memory_usage\u001b[39m\u001b[34m(self, index, deep)\u001b[39m\n\u001b[32m   5443\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmemory_usage\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m, deep: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m   5444\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5445\u001b[39m \u001b[33;03m    Return the memory usage of the Series.\u001b[39;00m\n\u001b[32m   5446\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5490\u001b[39m \u001b[33;03m    244\u001b[39;00m\n\u001b[32m   5491\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5492\u001b[39m     v = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_memory_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5493\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index:\n\u001b[32m   5494\u001b[39m         v += \u001b[38;5;28mself\u001b[39m.index.memory_usage(deep=deep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/dev-_bs4bCos/lib/python3.11/site-packages/pandas/core/base.py:1178\u001b[39m, in \u001b[36mIndexOpsMixin._memory_usage\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m is_object_dtype(\u001b[38;5;28mself\u001b[39m.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m PYPY:\n\u001b[32m   1177\u001b[39m     values = cast(np.ndarray, \u001b[38;5;28mself\u001b[39m._values)\n\u001b[32m-> \u001b[39m\u001b[32m1178\u001b[39m     v += \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmemory_usage_of_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m v\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load historic data (use Parquet for faster loading)\n",
    "historic_path = Path('../data/processed/historic_orders_2022_2024.parquet')\n",
    "\n",
    "if historic_path.exists():\n",
    "    print(f\"Loading historic data from {historic_path}...\")\n",
    "    df_historic = pd.read_parquet(historic_path)\n",
    "    print(f\"✓ Loaded {len(df_historic):,} records\")\n",
    "else:\n",
    "    # Try CSV.gz if Parquet doesn't exist\n",
    "    historic_path_csv = Path('../data/processed/historic_orders_2022_2024.csv.gz')\n",
    "    print(f\"Loading historic data from {historic_path_csv}...\")\n",
    "    df_historic = pd.read_csv(historic_path_csv, compression='gzip')\n",
    "    print(f\"✓ Loaded {len(df_historic):,} records\")\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Shape: {df_historic.shape}\")\n",
    "print(f\"  Date range: {df_historic['Datum.Tour'].min()} to {df_historic['Datum.Tour'].max()}\")\n",
    "print(f\"  Memory: {df_historic.memory_usage(deep=True).sum() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Monthly Aggregation by Betriebszentralen\n",
    "\n",
    "Aggregate historic data to monthly level grouped by the 14 Betriebszentralen (dispatch centers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating monthly aggregations...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure date column is datetime\n",
    "df_historic['Datum.Tour'] = pd.to_datetime(df_historic['Datum.Tour'])\n",
    "\n",
    "# Create year-month column\n",
    "df_historic['year_month'] = df_historic['Datum.Tour'].dt.to_period('M')\n",
    "\n",
    "# Find the order ID column (use the most complete identifier)\n",
    "order_id_cols = [\n",
    "  'NummerKomplett.Auftrag',  # Most complete\n",
    "  'Nummer.Auftrag',           # Order number\n",
    "  'Nummer.Hauptauftrag',      # Main order\n",
    "  'Auftragsschein-Nr.',       # Legacy name\n",
    "]\n",
    "\n",
    "order_id_col = None\n",
    "for col in order_id_cols:\n",
    "  if col in df_historic.columns:\n",
    "      order_id_col = col\n",
    "      print(f\"✓ Using order ID column: {col}\")\n",
    "      break\n",
    "\n",
    "if not order_id_col:\n",
    "  # Fallback: use index as count\n",
    "  print(\"⚠️  No order ID column found, using record count\")\n",
    "  df_historic['_order_count'] = 1\n",
    "  order_id_col = '_order_count'\n",
    "\n",
    "# Check if betriebszentrale_name exists\n",
    "if 'betriebszentrale_name' not in df_historic.columns:\n",
    "  print(\"⚠️  'betriebszentrale_name' not found, using 'Unknown'\")\n",
    "  df_historic['betriebszentrale_name'] = 'Unknown'\n",
    "\n",
    "# Check if carrier_type exists\n",
    "if 'carrier_type' not in df_historic.columns:\n",
    "  print(\"⚠️  'carrier_type' not found, classifying now...\")\n",
    "  # Quick carrier classification\n",
    "  def classify_carrier(carrier_num):\n",
    "      if pd.isna(carrier_num):\n",
    "          return 'unknown'\n",
    "      try:\n",
    "          carrier_num = float(str(carrier_num).replace('-', '').strip())\n",
    "          if carrier_num <= 8889:\n",
    "              return 'internal'\n",
    "          elif carrier_num >= 9000:\n",
    "              return 'external'\n",
    "      except:\n",
    "          pass\n",
    "      return 'unknown'\n",
    "\n",
    "  df_historic['carrier_type'] = df_historic['Nummer.Spedition'].apply(classify_carrier)\n",
    "\n",
    "# Fix: Convert distance column to numeric\n",
    "print(\"\\nPreparing columns for aggregation...\")\n",
    "\n",
    "# Convert distance to numeric (handles strings and errors)\n",
    "if 'Distanz_BE.Auftrag' in df_historic.columns:\n",
    "  df_historic['Distanz_BE.Auftrag'] = pd.to_numeric(\n",
    "      df_historic['Distanz_BE.Auftrag'],\n",
    "      errors='coerce'  # Convert invalid values to NaN\n",
    "  )\n",
    "  print(f\"  ✓ Converted Distanz_BE.Auftrag to numeric\")\n",
    "  print(f\"    Valid distances: {df_historic['Distanz_BE.Auftrag'].notna().sum():,}\")\n",
    "  print(f\"    Invalid/missing: {df_historic['Distanz_BE.Auftrag'].isna().sum():,}\")\n",
    "\n",
    "# Convert revenue column to numeric if exists\n",
    "if '∑ Einnahmen' in df_historic.columns:\n",
    "  df_historic['∑ Einnahmen'] = pd.to_numeric(\n",
    "      df_historic['∑ Einnahmen'],\n",
    "      errors='coerce'  # Convert invalid values to NaN\n",
    "  )\n",
    "  print(f\"  ✓ Converted ∑ Einnahmen (revenue) to numeric\")\n",
    "  print(f\"    Valid revenue: {df_historic['∑ Einnahmen'].notna().sum():,}\")\n",
    "  print(f\"    Non-zero revenue: {(df_historic['∑ Einnahmen'] > 0).sum():,}\")\n",
    "\n",
    "# Group by month and Betriebszentralen\n",
    "print(\"\\nAggregating by year-month and Betriebszentralen...\")\n",
    "\n",
    "agg_dict = {\n",
    "  order_id_col: 'count',  # Total orders\n",
    "  'Distanz_BE.Auftrag': 'sum',  # Total KM\n",
    "}\n",
    "\n",
    "# Add revenue if column exists\n",
    "if '∑ Einnahmen' in df_historic.columns:\n",
    "  agg_dict['∑ Einnahmen'] = 'sum'  # Total revenue\n",
    "  print(\"  ✓ Including revenue_total in aggregation\")\n",
    "\n",
    "monthly_agg = df_historic.groupby(['year_month',\n",
    "'betriebszentrale_name']).agg(agg_dict).reset_index()\n",
    "\n",
    "# Rename columns\n",
    "if '∑ Einnahmen' in df_historic.columns:\n",
    "  monthly_agg.columns = ['year_month', 'betriebszentrale', 'total_orders', 'total_km', 'revenue_total']\n",
    "else:\n",
    "  monthly_agg.columns = ['year_month', 'betriebszentrale', 'total_orders', 'total_km']\n",
    "\n",
    "# Add external drivers count\n",
    "external_count = df_historic[df_historic['carrier_type'] == 'external'].groupby(\n",
    "  ['year_month', 'betriebszentrale_name']\n",
    ").size().reset_index(name='external_drivers')\n",
    "\n",
    "monthly_agg = monthly_agg.merge(\n",
    "  external_count,\n",
    "  left_on=['year_month', 'betriebszentrale'],\n",
    "  right_on=['year_month', 'betriebszentrale_name'],\n",
    "  how='left'\n",
    ").drop(columns=['betriebszentrale_name'])\n",
    "\n",
    "# Add internal drivers count\n",
    "internal_count = df_historic[df_historic['carrier_type'] == 'internal'].groupby(\n",
    "  ['year_month', 'betriebszentrale_name']\n",
    ").size().reset_index(name='internal_drivers')\n",
    "\n",
    "monthly_agg = monthly_agg.merge(\n",
    "  internal_count,\n",
    "  left_on=['year_month', 'betriebszentrale'],\n",
    "  right_on=['year_month', 'betriebszentrale_name'],\n",
    "  how='left'\n",
    ").drop(columns=['betriebszentrale_name'])\n",
    "\n",
    "# Fill NaN with 0 for driver counts\n",
    "monthly_agg['external_drivers'] = monthly_agg['external_drivers'].fillna(0).astype(int)\n",
    "monthly_agg['internal_drivers'] = monthly_agg['internal_drivers'].fillna(0).astype(int)\n",
    "\n",
    "# Add order type breakdown if available\n",
    "if 'order_type' in df_historic.columns:\n",
    "  order_types = df_historic.groupby(['year_month', 'betriebszentrale_name',\n",
    "'order_type']).size().unstack(fill_value=0)\n",
    "  order_types = order_types.reset_index()\n",
    "  order_types.columns.name = None\n",
    "  order_types = order_types.rename(columns={'betriebszentrale_name': 'betriebszentrale'})\n",
    "\n",
    "  monthly_agg = monthly_agg.merge(order_types, on=['year_month', 'betriebszentrale'],\n",
    "how='left')\n",
    "\n",
    "# Convert year_month to datetime for easier plotting\n",
    "monthly_agg['date'] = monthly_agg['year_month'].dt.to_timestamp()\n",
    "\n",
    "print(f\"\\n✓ Monthly aggregation complete\")\n",
    "print(f\"  Total month-entity combinations: {len(monthly_agg):,}\")\n",
    "print(f\"  Unique months: {monthly_agg['year_month'].nunique()}\")\n",
    "print(f\"  Unique Betriebszentralen: {monthly_agg['betriebszentrale'].nunique()}\")\n",
    "print(f\"\\n  Date range: {monthly_agg['date'].min()} to {monthly_agg['date'].max()}\")\n",
    "\n",
    "# Preview\n",
    "print(\"\\nSample of aggregated data:\")\n",
    "print(monthly_agg.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ADD TOUR-BASED METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING TOUR DATA TO ADD ACTUAL KM AND TOUR COUNT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load tour data from tour assignment files\n",
    "print(\"\\nLoading tour data for 2022-2024...\")\n",
    "\n",
    "try:\n",
    "    # Load tour assignments for each year (direct file loading)\n",
    "    tour_data_list = []\n",
    "    for year in [2022, 2023, 2024]:\n",
    "        try:\n",
    "            tour_file = f'../data/raw/{year}/{year} QS Tourenaufstellung.xlsx'\n",
    "            print(f\"  Loading {tour_file}...\")\n",
    "            df_tour_year = pd.read_excel(tour_file)\n",
    "            df_tour_year['year'] = year\n",
    "            tour_data_list.append(df_tour_year)\n",
    "            print(f\"  ✓ Loaded {year}: {len(df_tour_year):,} tour records\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️  Could not load {year} tour data: {e}\")\n",
    "    \n",
    "    if tour_data_list:\n",
    "        df_tours = pd.concat(tour_data_list, ignore_index=True)\n",
    "        print(f\"\\n✓ Combined tour data: {len(df_tours):,} total tour records\")\n",
    "\n",
    "        # ====================================================================\n",
    "        # FIX: CONVERT NUMMER.TOUR TO INT64 FOR MATCHING WITH ORDER DATA\n",
    "        # ====================================================================\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"CONVERTING TOUR NUMBERS FOR COMPATIBILITY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"\\nBefore conversion:\")\n",
    "        print(f\"  dtype: {df_tours['Nummer.Tour'].dtype}\")\n",
    "        print(f\"  sample: {df_tours['Nummer.Tour'].head(3).tolist()}\")\n",
    "        \n",
    "        # Convert to numeric (handles string '782479' and '000002' formats)\n",
    "        df_tours['Nummer.Tour'] = pd.to_numeric(df_tours['Nummer.Tour'], errors='coerce')\n",
    "        \n",
    "        # Drop tours with invalid/missing tour numbers\n",
    "        invalid_count = df_tours['Nummer.Tour'].isna().sum()\n",
    "        if invalid_count > 0:\n",
    "            print(f\"\\n⚠️  Dropping {invalid_count:,} tours with invalid tour numbers\")\n",
    "            df_tours = df_tours[df_tours['Nummer.Tour'].notna()]\n",
    "        \n",
    "        # Convert to int64\n",
    "        df_tours['Nummer.Tour'] = df_tours['Nummer.Tour'].astype('int64')\n",
    "        print(f\"\\nAfter conversion:\")\n",
    "        print(f\"  dtype: {df_tours['Nummer.Tour'].dtype}\")\n",
    "        print(f\"  sample: {df_tours['Nummer.Tour'].head(3).tolist()}\")\n",
    "        print(f\"  ✓ Tour numbers normalized to int64 for matching\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        # ====================================================================\n",
    "\n",
    "        \n",
    "        # Fix: Date column is \"Datum Tour\" (with space) not \"Datum.Tour\"\n",
    "        date_col = 'Datum Tour' if 'Datum Tour' in df_tours.columns else 'Datum.Tour'\n",
    "        df_tours[date_col] = pd.to_datetime(df_tours[date_col])\n",
    "        df_tours['year_month'] = df_tours[date_col].dt.to_period('M')\n",
    "        \n",
    "        # Fix: Use \"Soll KM PraCar\" instead of \"IstKm.Tour\" (IstKm is empty)\n",
    "        km_col = 'Soll KM PraCar' if 'Soll KM PraCar' in df_tours.columns else 'IstKm.Tour'\n",
    "        df_tours[km_col] = pd.to_numeric(df_tours[km_col], errors='coerce')\n",
    "        \n",
    "        print(f\"\\nColumn mapping:\")\n",
    "        print(f\"  Date column: {date_col}\")\n",
    "        print(f\"  KM column: {km_col}\")\n",
    "        print(f\"  Tour ID column: Nummer.Tour\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # FIX: MAP TOURS TO BETRIEBSZENTRALEN BEFORE AGGREGATION\n",
    "        # ====================================================================\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"MAPPING TOURS TO BETRIEBSZENTRALEN\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Create tour → Betriebszentrale mapping from historic order data\n",
    "        print(\"\\nCreating tour-to-Betriebszentrale mapping from order data...\")\n",
    "        \n",
    "        # Use Nummer.Tour from historic data to map to betriebszentrale_name\n",
    "        if 'Nummer.Tour' in df_historic.columns and 'betriebszentrale_name' in df_historic.columns:\n",
    "            # Take the most common Betriebszentrale for each tour (majority vote)\n",
    "            tour_to_bz = df_historic.groupby('Nummer.Tour')['betriebszentrale_name'].agg(\n",
    "                lambda x: x.value_counts().index[0] if len(x) > 0 else 'Unknown'\n",
    "            ).reset_index()\n",
    "            tour_to_bz.columns = ['Nummer.Tour', 'betriebszentrale']\n",
    "            \n",
    "            # FIX: Convert tour_to_bz tour numbers to int64 to match df_tours\n",
    "            tour_to_bz['Nummer.Tour'] = pd.to_numeric(tour_to_bz['Nummer.Tour'], errors='coerce')\n",
    "            tour_to_bz = tour_to_bz[tour_to_bz['Nummer.Tour'].notna()]  # Remove invalid tour numbers\n",
    "            tour_to_bz['Nummer.Tour'] = tour_to_bz['Nummer.Tour'].astype('int64')\n",
    "            \n",
    "            \n",
    "            print(f\"✓ Created mapping for {len(tour_to_bz):,} unique tours\")\n",
    "            print(f\"  Example: {tour_to_bz.head(3).to_dict('records')}\")\n",
    "            \n",
    "            # Merge mapping with tour data\n",
    "            print(f\"\\nMerging Betriebszentrale mapping with tour data...\")\n",
    "            df_tours = df_tours.merge(tour_to_bz, on='Nummer.Tour', how='left')\n",
    "\n",
    "            # Calculate match rate to verify type fix worked\n",
    "            match_rate = (df_tours['betriebszentrale'].notna().sum() / len(df_tours)) * 100\n",
    "            print(f\"  Match rate: {match_rate:.1f}% ({df_tours['betriebszentrale'].notna().sum():,} / {len(df_tours):,} tours)\")\n",
    "            if match_rate < 95:\n",
    "                print(f\"  ⚠️  WARNING: Low match rate! Check tour number compatibility\")\n",
    "            else:\n",
    "                print(f\"  ✓ Excellent match rate - type conversion successful!\")\n",
    "\n",
    "            \n",
    "            # Fill unmapped tours with 'Unknown'\n",
    "            unmapped_count = df_tours['betriebszentrale'].isna().sum()\n",
    "            if unmapped_count > 0:\n",
    "                print(f\"  ⚠️  {unmapped_count:,} tours not mapped to any Betriebszentrale (will use 'Unknown')\")\n",
    "                df_tours['betriebszentrale'] = df_tours['betriebszentrale'].fillna('Unknown')\n",
    "            \n",
    "            print(f\"✓ Tour mapping complete\")\n",
    "            print(f\"  Tours by Betriebszentrale:\")\n",
    "            bz_counts = df_tours['betriebszentrale'].value_counts()\n",
    "            for bz, count in bz_counts.head(15).items():\n",
    "                print(f\"    {bz}: {count:,} tours\")\n",
    "        else:\n",
    "            print(f\"⚠️  Cannot map tours to Betriebszentralen (missing columns)\")\n",
    "            print(f\"   Using 'Unknown' for all tours\")\n",
    "            df_tours['betriebszentrale'] = 'Unknown'\n",
    "        \n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Aggregate tour data to monthly level BY BETRIEBSZENTRALE\n",
    "        print(\"Aggregating tour data to monthly level by Betriebszentrale...\")\n",
    "        tour_monthly = df_tours.groupby(['year_month', 'betriebszentrale']).agg({\n",
    "            'Nummer.Tour': 'nunique',  # Count unique tours\n",
    "            km_col: 'sum'               # Sum actual kilometers from PraCar\n",
    "        }).reset_index()\n",
    "        \n",
    "        tour_monthly.columns = ['year_month', 'betriebszentrale', 'total_tours', 'total_km_actual']\n",
    "        \n",
    "        print(f\"✓ Tour aggregation complete\")\n",
    "        print(f\"  Month-Betriebszentrale combinations: {len(tour_monthly)}\")\n",
    "        print(f\"  Total unique tours: {tour_monthly['total_tours'].sum():,.0f}\")\n",
    "        print(f\"  Total actual KM: {tour_monthly['total_km_actual'].sum():,.0f}\")\n",
    "        \n",
    "        # Rename existing total_km to total_km_billed\n",
    "        print(\"\\nRenaming columns:\")\n",
    "        print(\"  total_km → total_km_billed (order-based billing distances)\")\n",
    "        monthly_agg = monthly_agg.rename(columns={'total_km': 'total_km_billed'})\n",
    "        \n",
    "        # Merge tour data with order-based aggregation (BY BOTH year_month AND betriebszentrale)\n",
    "        print(\"\\nMerging tour metrics with order aggregation...\")\n",
    "        monthly_agg = monthly_agg.merge(\n",
    "            tour_monthly, \n",
    "            on=['year_month', 'betriebszentrale'],  # KEY FIX: Merge on BOTH columns\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Fill missing tour data with NaN (not 0, to distinguish missing from zero)\n",
    "        print(f\"✓ Merge complete\")\n",
    "        matched = monthly_agg['total_tours'].notna().sum()\n",
    "        missing = monthly_agg['total_tours'].isna().sum()\n",
    "        print(f\"  Matched: {matched} month-entity combinations\")\n",
    "        print(f\"  Missing tour data: {missing} month-entity combinations\")\n",
    "        \n",
    "        # Calculate KM efficiency ratio (where both values exist)\n",
    "        monthly_agg['km_efficiency'] = monthly_agg['total_km_actual'] / monthly_agg['total_km_billed']\n",
    "        \n",
    "        print(f\"\\n✓ New metrics added:\")\n",
    "        print(f\"  • total_tours: Tour count per month BY BETRIEBSZENTRALE\")\n",
    "        print(f\"  • total_km_actual: Actual driven km from tours BY BETRIEBSZENTRALE\")\n",
    "        print(f\"  • total_km_billed: Order-based billing km (renamed from total_km)\")\n",
    "        print(f\"  • km_efficiency: Ratio of actual/billed km\")\n",
    "        \n",
    "        # Show sample efficiency metrics\n",
    "        if monthly_agg['km_efficiency'].notna().sum() > 0:\n",
    "            efficiency_sample = monthly_agg[monthly_agg['km_efficiency'].notna()]['km_efficiency'].describe()\n",
    "            print(f\"\\nKM Efficiency Statistics (actual/billed):\")\n",
    "            print(f\"  Mean: {efficiency_sample['mean']:.2%}\")\n",
    "            print(f\"  Median: {efficiency_sample['50%']:.2%}\")\n",
    "            print(f\"  Min: {efficiency_sample['min']:.2%}\")\n",
    "            print(f\"  Max: {efficiency_sample['max']:.2%}\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️  No KM efficiency data available (missing actual or billed KM)\")\n",
    "\n",
    "        # ====================================================================\n",
    "        # ADD VEHICLE COST METRICS (WITH BETRIEBSZENTRALE SPLIT)\n",
    "        # ====================================================================\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"CALCULATING VEHICLE COST METRICS BY BETRIEBSZENTRALE\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        # Check for required cost columns\n",
    "        cost_columns = {\n",
    "            'km_cost': 'PC KM Kosten',\n",
    "            'time_cost': 'PC Minuten Kosten',\n",
    "            'ist_zeit': 'IST Zeit PraCar'\n",
    "        }\n",
    "\n",
    "        missing_cols = []\n",
    "        for key, col in cost_columns.items():\n",
    "            if col not in df_tours.columns:\n",
    "                missing_cols.append(col)\n",
    "\n",
    "        if missing_cols:\n",
    "            print(f\"⚠️  Missing required columns for cost calculation: {missing_cols}\")\n",
    "            print(f\"   Available columns: {df_tours.columns.tolist()}\")\n",
    "            print(f\"   Skipping cost metrics...\")\n",
    "        else:\n",
    "            print(f\"\\n✓ Found required cost columns:\")\n",
    "            print(f\"  • {cost_columns['km_cost']} (KM cost rate)\")\n",
    "            print(f\"  • {cost_columns['time_cost']} (Minute cost rate)\")\n",
    "            print(f\"  • {cost_columns['ist_zeit']} (Actual time in hours)\")\n",
    "\n",
    "            # Convert cost columns to numeric\n",
    "            df_tours[cost_columns['km_cost']] = pd.to_numeric(\n",
    "                df_tours[cost_columns['km_cost']], errors='coerce'\n",
    "            )\n",
    "            df_tours[cost_columns['time_cost']] = pd.to_numeric(\n",
    "                df_tours[cost_columns['time_cost']], errors='coerce'\n",
    "            )\n",
    "            df_tours[cost_columns['ist_zeit']] = pd.to_numeric(\n",
    "                df_tours[cost_columns['ist_zeit']], errors='coerce'\n",
    "            )\n",
    "\n",
    "            print(f\"\\nCalculating cost components...\")\n",
    "\n",
    "            # Calculate KM cost component: Actual KM × KM Cost Rate\n",
    "            df_tours['km_cost_component'] = (\n",
    "                df_tours[km_col] * df_tours[cost_columns['km_cost']]\n",
    "            )\n",
    "\n",
    "            # Calculate Time cost component: Hours × 60 minutes × Minute Cost Rate\n",
    "            df_tours['time_cost_component'] = (\n",
    "                df_tours[cost_columns['ist_zeit']] * 60 * df_tours[cost_columns['time_cost']]\n",
    "            )\n",
    "\n",
    "            # Calculate Total vehicle cost\n",
    "            df_tours['total_vehicle_cost'] = (\n",
    "                df_tours['km_cost_component'] + df_tours['time_cost_component']\n",
    "            )\n",
    "\n",
    "            print(f\"✓ Cost calculation complete\")\n",
    "            print(f\"  Valid KM costs: {df_tours['km_cost_component'].notna().sum():,}\")\n",
    "            print(f\"  Valid time costs: {df_tours['time_cost_component'].notna().sum():,}\")\n",
    "            print(f\"  Valid total costs: {df_tours['total_vehicle_cost'].notna().sum():,}\")\n",
    "\n",
    "            # Aggregate costs to monthly level BY BETRIEBSZENTRALE (KEY FIX!)\n",
    "            print(f\"\\nAggregating costs to monthly level by Betriebszentrale...\")\n",
    "            cost_monthly = df_tours.groupby(['year_month', 'betriebszentrale']).agg({\n",
    "                'km_cost_component': 'sum',\n",
    "                'time_cost_component': 'sum',\n",
    "                'total_vehicle_cost': 'sum'\n",
    "            }).reset_index()\n",
    "\n",
    "            cost_monthly.columns = [\n",
    "                'year_month',\n",
    "                'betriebszentrale',  # KEY FIX: Include betriebszentrale\n",
    "                'vehicle_km_cost',\n",
    "                'vehicle_time_cost',\n",
    "                'total_vehicle_cost'\n",
    "            ]\n",
    "\n",
    "            print(f\"✓ Cost aggregation complete\")\n",
    "            print(f\"  Month-Betriebszentrale combinations: {len(cost_monthly)}\")\n",
    "\n",
    "            # Merge cost data with monthly aggregation (BY BOTH year_month AND betriebszentrale)\n",
    "            print(f\"\\nMerging cost metrics with monthly aggregation...\")\n",
    "            monthly_agg = monthly_agg.merge(\n",
    "                cost_monthly, \n",
    "                on=['year_month', 'betriebszentrale'],  # KEY FIX: Merge on BOTH columns\n",
    "                how='left'\n",
    "            )\n",
    "\n",
    "            matched_costs = monthly_agg['total_vehicle_cost'].notna().sum()\n",
    "            print(f\"✓ Cost merge complete\")\n",
    "            print(f\"  Matched: {matched_costs} month-entity combinations with cost data\")\n",
    "\n",
    "            # Show cost statistics (COMPANY-WIDE aggregated correctly from branches)\n",
    "            print(f\"\\nVehicle Cost Statistics (2022-2024):\")\n",
    "            print(f\"  Total KM cost: CHF {cost_monthly['vehicle_km_cost'].sum():,.2f}\")\n",
    "            print(f\"  Total time cost: CHF {cost_monthly['vehicle_time_cost'].sum():,.2f}\")\n",
    "            print(f\"  Total vehicle cost: CHF {cost_monthly['total_vehicle_cost'].sum():,.2f}\")\n",
    "            print(f\"\\n  Average per month:\")\n",
    "            print(f\"    KM cost: CHF {cost_monthly['vehicle_km_cost'].sum() / 36:,.2f}\")\n",
    "            print(f\"    Time cost: CHF {cost_monthly['vehicle_time_cost'].sum() / 36:,.2f}\")\n",
    "            print(f\"    Total cost: CHF {cost_monthly['total_vehicle_cost'].sum() / 36:,.2f}\")\n",
    "\n",
    "            # Calculate cost per tour\n",
    "            total_tours = tour_monthly['total_tours'].sum()\n",
    "            total_cost = cost_monthly['total_vehicle_cost'].sum()\n",
    "            cost_per_tour = total_cost / total_tours if total_tours > 0 else 0\n",
    "            print(f\"\\n  Average cost per tour: CHF {cost_per_tour:.2f}\")\n",
    "\n",
    "            print(f\"\\n✓ New cost metrics added (BY BETRIEBSZENTRALE):\")\n",
    "            print(f\"  • vehicle_km_cost: KM-based transportation cost\")\n",
    "            print(f\"  • vehicle_time_cost: Time-based transportation cost\")\n",
    "            print(f\"  • total_vehicle_cost: Total vehicle operational cost\")\n",
    "            print(f\"\\n  ⚠️  IMPORTANT: Costs are now correctly split by Betriebszentrale!\")\n",
    "            print(f\"     When aggregating to company level, costs will sum correctly (no duplication)\")\n",
    "\n",
    "    else:\n",
    "        print(\"⚠️  No tour data loaded - skipping tour metrics\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"\\n⚠️  Error loading tour data: {e}\")\n",
    "    print(f\"   Traceback:\")\n",
    "    traceback.print_exc()\n",
    "    print(\"   Continuing without tour metrics...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Load June 2025 Data\n",
    "\n",
    "Load the already-processed June 2025 data from notebook 04."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have processed June 2025 aggregated data\n",
    "june_2025_path = Path('../data/processed/monthly_aggregated.csv')\n",
    "\n",
    "if june_2025_path.exists():\n",
    "    print(f\"Loading June 2025 aggregated data from {june_2025_path}...\")\n",
    "    df_june_2025 = pd.read_csv(june_2025_path)\n",
    "    \n",
    "    # Filter for June 2025 only\n",
    "    if 'year_month' in df_june_2025.columns:\n",
    "        df_june_2025['year_month'] = pd.to_datetime(df_june_2025['year_month']).dt.to_period('M')\n",
    "        df_june_2025 = df_june_2025[df_june_2025['year_month'] == '2025-06']\n",
    "    \n",
    "    print(f\"✓ Loaded June 2025: {len(df_june_2025)} Betriebszentralen\")\n",
    "    print(f\"  Columns: {list(df_june_2025.columns)}\")\n",
    "else:\n",
    "    print(\"⚠️  June 2025 aggregated data not found.\")\n",
    "    print(\"    You may need to run notebook 04 first to create monthly_aggregated.csv\")\n",
    "    df_june_2025 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Merge Historic and June 2025 Data\n",
    "\n",
    "Combine into single time series dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Finalize Dataset (Historic Only)\n",
    "print(\"Finalizing dataset (2022-2024 only)...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use the monthly_agg from Section 2\n",
    "df_full_ts = monthly_agg.copy()\n",
    "\n",
    "print(f\"\\n✓ Dataset finalized!\")\n",
    "print(f\"  Total records: {len(df_full_ts):,}\")\n",
    "print(f\"  Date range: {df_full_ts['date'].min()} to {df_full_ts['date'].max()}\")\n",
    "print(f\"  Total months: {df_full_ts['year_month'].nunique()}\")\n",
    "print(f\"  Betriebszentralen: {df_full_ts['betriebszentrale'].nunique()}\")\n",
    "\n",
    "print(f\"\\nAvailable metrics:\")\n",
    "metric_cols = [col for col in df_full_ts.columns\n",
    "             if col not in ['year_month', 'date', 'betriebszentrale']]\n",
    "for col in metric_cols:\n",
    "  non_null = df_full_ts[col].notna().sum()\n",
    "  print(f\"  • {col}: {non_null:,} non-null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INTEGRATE WORKING DAYS DATA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTEGRATING WORKING DAYS DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "working_days_path = Path('../data/raw/TRAVECO_Arbeitstage_2022-laufend_für gopf.com_hb v1.xlsx')\n",
    "\n",
    "try:\n",
    "    # Read Excel file\n",
    "    df_working_days_wide = pd.read_excel(working_days_path, sheet_name='Tabelle1')\n",
    "    \n",
    "    print(f\"✓ Loaded working days file: {df_working_days_wide.shape}\")\n",
    "    \n",
    "    # Transform from wide format to long format\n",
    "    month_mapping = {\n",
    "        'Januar': 1, 'Februar': 2, 'März': 3, 'April': 4,\n",
    "        'Mai': 5, 'Juni': 6, 'Juli': 7, 'August': 8,\n",
    "        'September': 9, 'Oktober': 10, 'November': 11, 'Dezember': 12\n",
    "    }\n",
    "    \n",
    "    # Melt from wide to long\n",
    "    id_vars = ['Jahr']\n",
    "    value_vars = [col for col in df_working_days_wide.columns if col in month_mapping.keys()]\n",
    "    \n",
    "    df_working_days = df_working_days_wide.melt(\n",
    "        id_vars=id_vars,\n",
    "        value_vars=value_vars,\n",
    "        var_name='month_name',\n",
    "        value_name='working_days'\n",
    "    )\n",
    "    \n",
    "    # Map German month names to numbers\n",
    "    df_working_days['month'] = df_working_days['month_name'].map(month_mapping)\n",
    "    df_working_days = df_working_days.rename(columns={'Jahr': 'year'})\n",
    "    df_working_days = df_working_days[['year', 'month', 'working_days']].copy()\n",
    "    \n",
    "    # Remove rows with NaN working_days (future months)\n",
    "    df_working_days = df_working_days.dropna(subset=['working_days'])\n",
    "    df_working_days['working_days'] = df_working_days['working_days'].astype(int)\n",
    "    df_working_days['year'] = df_working_days['year'].astype(int)\n",
    "    df_working_days['month'] = df_working_days['month'].astype(int)\n",
    "    \n",
    "    print(f\"✓ Transformed to long format: {len(df_working_days)} month records\")\n",
    "    print(f\"  Date range: {df_working_days['year'].min()}-{df_working_days['month'].min():02d} to {df_working_days['year'].max()}-{df_working_days['month'].max():02d}\")\n",
    "    print(f\"  Working days range: {df_working_days['working_days'].min()}-{df_working_days['working_days'].max()} days/month\")\n",
    "    \n",
    "    # Join with monthly aggregated data (df_full_ts already created in cell 10)\n",
    "    print(\"\\nJoining working days with time series data...\")\n",
    "    \n",
    "    # Extract year and month from df_full_ts if not present\n",
    "    if 'year' not in df_full_ts.columns or 'month' not in df_full_ts.columns:\n",
    "        df_full_ts['year'] = df_full_ts['date'].dt.year\n",
    "        df_full_ts['month'] = df_full_ts['date'].dt.month\n",
    "    \n",
    "    # Merge\n",
    "    df_full_ts = df_full_ts.merge(\n",
    "        df_working_days[['year', 'month', 'working_days']],\n",
    "        on=['year', 'month'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Check join success\n",
    "    missing_working_days = df_full_ts['working_days'].isnull().sum()\n",
    "    if missing_working_days > 0:\n",
    "        print(f\"  ⚠️  {missing_working_days} months missing working days data\")\n",
    "        print(\"     These months are outside the working days data range\")\n",
    "    else:\n",
    "        print(f\"  ✓ All {len(df_full_ts)} month-entity combinations matched\")\n",
    "    \n",
    "    print(f\"\\n✓ Working days integration complete!\")\n",
    "    print(f\"  Working days column added to df_full_ts\")\n",
    "    print(f\"  This enables calendar-normalized forecasting metrics\")\n",
    "    print(f\"\\n  Note: Per-working-day metrics can be calculated when needed\")\n",
    "    print(f\"        (e.g., orders_per_working_day = total_orders / working_days)\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"⚠️  Working days file not found: {working_days_path}\")\n",
    "    print(\"   Skipping working days integration...\")\n",
    "    print(\"   Forecasting will use absolute metrics only\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Error loading working days: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check what columns we have in the merged dataset\n",
    "print(\"\\nColumns in merged dataset:\")\n",
    "print(df_full_ts.columns.tolist())\n",
    "\n",
    "print(f\"\\nSample of merged data:\")\n",
    "print(df_full_ts.head(10))\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(df_full_ts.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Data Completeness Check\n",
    "\n",
    "Verify we have complete time series for each Betriebszentralen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time Series Completeness Check\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Expected months\n",
    "all_months = pd.period_range(\n",
    "    start=df_full_ts['year_month'].min(), \n",
    "    end=df_full_ts['year_month'].max(), \n",
    "    freq='M'\n",
    ")\n",
    "print(f\"\\nExpected months: {len(all_months)}\")\n",
    "print(f\"  From: {all_months[0]}\")\n",
    "print(f\"  To: {all_months[-1]}\")\n",
    "\n",
    "# Check completeness for each Betriebszentralen\n",
    "print(f\"\\nCompleteness by Betriebszentralen:\")\n",
    "print(f\"{'Betriebszentralen':<30} {'Months':>8} {'Complete?':>12}\")\n",
    "print(\"-\" * 52)\n",
    "\n",
    "for bz in sorted(df_full_ts['betriebszentrale'].unique()):\n",
    "    bz_data = df_full_ts[df_full_ts['betriebszentrale'] == bz]\n",
    "    month_count = bz_data['year_month'].nunique()\n",
    "    is_complete = \"✓\" if month_count == len(all_months) else \"⚠️  Missing\"\n",
    "    print(f\"{bz:<30} {month_count:>8} {is_complete:>12}\")\n",
    "\n",
    "# Check for missing months\n",
    "missing_data = []\n",
    "for bz in df_full_ts['betriebszentrale'].unique():\n",
    "    bz_months = set(df_full_ts[df_full_ts['betriebszentrale'] == bz]['year_month'])\n",
    "    missing_months = set(all_months) - bz_months\n",
    "    if missing_months:\n",
    "        missing_data.append((bz, sorted(missing_months)))\n",
    "\n",
    "if missing_data:\n",
    "    print(f\"\\n⚠️  Missing Data Details:\")\n",
    "    for bz, months in missing_data:\n",
    "        print(f\"  {bz}: {len(months)} missing months\")\n",
    "        if len(months) <= 5:\n",
    "            print(f\"    → {', '.join(str(m) for m in months)}\")\n",
    "else:\n",
    "    print(f\"\\n✓ All Betriebszentralen have complete time series!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter: Remove \"Unknown\" Betriebszentralen\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FILTERING: Remove Unknown Betriebszentralen\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Count before\n",
    "before_count = len(df_full_ts)\n",
    "unknown_count = len(df_full_ts[df_full_ts['betriebszentrale'] == 'Unknown'])\n",
    "\n",
    "# Filter out Unknown\n",
    "df_full_ts = df_full_ts[df_full_ts['betriebszentrale'] != 'Unknown'].copy()\n",
    "\n",
    "# Count after\n",
    "after_count = len(df_full_ts)\n",
    "\n",
    "print(f\"\\n✓ Filtering complete:\")\n",
    "print(f\"  Before: {before_count:,} records\")\n",
    "print(f\"  Removed (Unknown): {unknown_count:,} records\")\n",
    "print(f\"  After: {after_count:,} records\")\n",
    "print(f\"  Retention: {(after_count/before_count)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nFinal entities ({df_full_ts['betriebszentrale'].nunique()}):\")\n",
    "for entity in sorted(df_full_ts['betriebszentrale'].unique()):\n",
    "  month_count = df_full_ts[df_full_ts['betriebszentrale'] == entity]['year_month'].nunique()\n",
    "  print(f\"  • {entity}: {month_count} months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Aggregate to Company Level\n",
    "\n",
    "Create company-wide time series (sum across all Betriebszentralen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating company-level aggregation...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define aggregation dict - UPDATED for 10 metrics\n",
    "agg_cols = {\n",
    "    'total_orders': 'sum',\n",
    "    'external_drivers': 'sum',\n",
    "    'internal_drivers': 'sum'\n",
    "}\n",
    "\n",
    "# Add working_days (use 'first' since it's the same for all Betriebszentralen in a given month)\n",
    "if 'working_days' in df_full_ts.columns:\n",
    "    agg_cols['working_days'] = 'first'\n",
    "    print(\"  ✓ Including working_days in company aggregation\")\n",
    "\n",
    "# Add revenue if it exists in the dataframe\n",
    "if 'revenue_total' in df_full_ts.columns:\n",
    "    agg_cols['revenue_total'] = 'sum'\n",
    "    print(\"  ✓ Including revenue_total in company aggregation\")\n",
    "\n",
    "# Add KM metrics (billed and actual) - UPDATED\n",
    "if 'total_km_billed' in df_full_ts.columns:\n",
    "    agg_cols['total_km_billed'] = 'sum'\n",
    "    print(\"  ✓ Including total_km_billed (order-based billing KM)\")\n",
    "elif 'total_km' in df_full_ts.columns:\n",
    "    agg_cols['total_km'] = 'sum'\n",
    "    print(\"  ⚠️  Using total_km (not yet renamed)\")\n",
    "\n",
    "if 'total_km_actual' in df_full_ts.columns:\n",
    "    agg_cols['total_km_actual'] = 'sum'\n",
    "    print(\"  ✓ Including total_km_actual (tour-based actual KM)\")\n",
    "\n",
    "# Add tour count\n",
    "if 'total_tours' in df_full_ts.columns:\n",
    "    agg_cols['total_tours'] = 'sum'\n",
    "    print(\"  ✓ Including total_tours (unique tour count)\")\n",
    "\n",
    "# Add cost metrics - NEW (7 → 10 metrics)\n",
    "if 'vehicle_km_cost' in df_full_ts.columns:\n",
    "    agg_cols['vehicle_km_cost'] = 'sum'\n",
    "    print(\"  ✓ Including vehicle_km_cost (KM-based transportation cost)\")\n",
    "\n",
    "if 'vehicle_time_cost' in df_full_ts.columns:\n",
    "    agg_cols['vehicle_time_cost'] = 'sum'\n",
    "    print(\"  ✓ Including vehicle_time_cost (Time-based transportation cost)\")\n",
    "\n",
    "if 'total_vehicle_cost' in df_full_ts.columns:\n",
    "    agg_cols['total_vehicle_cost'] = 'sum'\n",
    "    print(\"  ✓ Including total_vehicle_cost (Total vehicle operational cost)\")\n",
    "\n",
    "# Aggregate across all Betriebszentralen\n",
    "company_ts = df_full_ts.groupby('year_month').agg(agg_cols).reset_index()\n",
    "\n",
    "# Add order types if available\n",
    "if 'Delivery' in df_full_ts.columns:\n",
    "    order_type_cols = [col for col in df_full_ts.columns if col in ['Delivery', 'Pickup/Multi-leg', 'Leergut', 'Retoure/Abholung']]\n",
    "    order_type_sum = df_full_ts.groupby('year_month')[order_type_cols].sum().reset_index()\n",
    "    company_ts = company_ts.merge(order_type_sum, on='year_month', how='left')\n",
    "\n",
    "# Convert to datetime\n",
    "company_ts['date'] = company_ts['year_month'].dt.to_timestamp()\n",
    "\n",
    "# Calculate total drivers\n",
    "company_ts['total_drivers'] = company_ts['external_drivers'] + company_ts['internal_drivers']\n",
    "\n",
    "# Calculate KM per order (use billed if available, otherwise use total_km)\n",
    "if 'total_km_billed' in company_ts.columns:\n",
    "    company_ts['km_per_order'] = company_ts['total_km_billed'] / company_ts['total_orders']\n",
    "elif 'total_km' in company_ts.columns:\n",
    "    company_ts['km_per_order'] = company_ts['total_km'] / company_ts['total_orders']\n",
    "\n",
    "# Calculate KM efficiency if we have both billed and actual\n",
    "if 'total_km_actual' in company_ts.columns and 'total_km_billed' in company_ts.columns:\n",
    "    company_ts['km_efficiency'] = company_ts['total_km_actual'] / company_ts['total_km_billed']\n",
    "    print(f\"  ✓ Calculated km_efficiency metric (actual/billed)\")\n",
    "\n",
    "# Calculate revenue per order if revenue exists\n",
    "if 'revenue_total' in company_ts.columns:\n",
    "    company_ts['revenue_per_order'] = company_ts['revenue_total'] / company_ts['total_orders']\n",
    "    print(f\"  ✓ Calculated revenue_per_order metric\")\n",
    "\n",
    "# Calculate cost per order if cost exists - NEW\n",
    "if 'total_vehicle_cost' in company_ts.columns:\n",
    "    company_ts['cost_per_order'] = company_ts['total_vehicle_cost'] / company_ts['total_orders']\n",
    "    print(f\"  ✓ Calculated cost_per_order metric\")\n",
    "\n",
    "# Calculate profit margin if we have both revenue and cost - NEW\n",
    "if 'revenue_total' in company_ts.columns and 'total_vehicle_cost' in company_ts.columns:\n",
    "    company_ts['profit_margin'] = (\n",
    "        (company_ts['revenue_total'] - company_ts['total_vehicle_cost']) / \n",
    "        company_ts['revenue_total']\n",
    "    ) * 100  # As percentage\n",
    "    print(f\"  ✓ Calculated profit_margin metric (%)\")\n",
    "\n",
    "print(f\"\\n✓ Company-level time series created\")\n",
    "print(f\"  Months: {len(company_ts)}\")\n",
    "print(f\"  Date range: {company_ts['date'].min()} to {company_ts['date'].max()}\")\n",
    "\n",
    "# Show metrics including all new ones\n",
    "metrics_to_show = ['total_orders']\n",
    "if 'total_km_billed' in company_ts.columns:\n",
    "    metrics_to_show.append('total_km_billed')\n",
    "elif 'total_km' in company_ts.columns:\n",
    "    metrics_to_show.append('total_km')\n",
    "if 'total_km_actual' in company_ts.columns:\n",
    "    metrics_to_show.append('total_km_actual')\n",
    "if 'total_tours' in company_ts.columns:\n",
    "    metrics_to_show.append('total_tours')\n",
    "metrics_to_show.append('total_drivers')\n",
    "if 'revenue_total' in company_ts.columns:\n",
    "    metrics_to_show.append('revenue_total')\n",
    "# Add cost metrics to summary\n",
    "if 'vehicle_km_cost' in company_ts.columns:\n",
    "    metrics_to_show.append('vehicle_km_cost')\n",
    "if 'vehicle_time_cost' in company_ts.columns:\n",
    "    metrics_to_show.append('vehicle_time_cost')\n",
    "if 'total_vehicle_cost' in company_ts.columns:\n",
    "    metrics_to_show.append('total_vehicle_cost')\n",
    "    \n",
    "print(f\"\\nSummary statistics:\")\n",
    "print(company_ts[metrics_to_show].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Time Series Visualization\n",
    "\n",
    "Visualize key metrics over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-panel time series dashboard - UPDATED with 5 panels including costs\n",
    "fig = make_subplots(\n",
    "    rows=5, cols=1,\n",
    "    subplot_titles=(\n",
    "        'Total Orders Over Time',\n",
    "        'Total Kilometers: Billed vs Actual',\n",
    "        'Total Tours Over Time',\n",
    "        'Vehicle Costs Over Time (Revenue vs Cost)',\n",
    "        'External vs Internal Drivers'\n",
    "    ),\n",
    "    vertical_spacing=0.08\n",
    ")\n",
    "\n",
    "# 1. Total Orders\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=company_ts['date'],\n",
    "        y=company_ts['total_orders'],\n",
    "        mode='lines+markers',\n",
    "        name='Total Orders',\n",
    "        line=dict(color='#1f77b4', width=2)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Total KM - Billed vs Actual\n",
    "if 'total_km_billed' in company_ts.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=company_ts['date'],\n",
    "            y=company_ts['total_km_billed'],\n",
    "            mode='lines+markers',\n",
    "            name='KM Billed (Orders)',\n",
    "            line=dict(color='#ff7f0e', width=2),\n",
    "            legendgroup='km'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "if 'total_km_actual' in company_ts.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=company_ts['date'],\n",
    "            y=company_ts['total_km_actual'],\n",
    "            mode='lines+markers',\n",
    "            name='KM Actual (Tours)',\n",
    "            line=dict(color='#2ca02c', width=2, dash='dash'),\n",
    "            legendgroup='km'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Fallback if only total_km exists\n",
    "if 'total_km_billed' not in company_ts.columns and 'total_km' in company_ts.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=company_ts['date'],\n",
    "            y=company_ts['total_km'],\n",
    "            mode='lines+markers',\n",
    "            name='Total KM',\n",
    "            line=dict(color='#ff7f0e', width=2)\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# 3. Total Tours\n",
    "if 'total_tours' in company_ts.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=company_ts['date'],\n",
    "            y=company_ts['total_tours'],\n",
    "            mode='lines+markers',\n",
    "            name='Total Tours',\n",
    "            line=dict(color='#9467bd', width=2)\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "\n",
    "# 4. Vehicle Costs (NEW - Revenue vs Cost comparison)\n",
    "if 'revenue_total' in company_ts.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=company_ts['date'],\n",
    "            y=company_ts['revenue_total'],\n",
    "            mode='lines+markers',\n",
    "            name='Revenue Total',\n",
    "            line=dict(color='#2ca02c', width=2),\n",
    "            legendgroup='finance'\n",
    "        ),\n",
    "        row=4, col=1\n",
    "    )\n",
    "\n",
    "if 'total_vehicle_cost' in company_ts.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=company_ts['date'],\n",
    "            y=company_ts['total_vehicle_cost'],\n",
    "            mode='lines+markers',\n",
    "            name='Total Vehicle Cost',\n",
    "            line=dict(color='#d62728', width=2),\n",
    "            legendgroup='finance'\n",
    "        ),\n",
    "        row=4, col=1\n",
    "    )\n",
    "    \n",
    "    # Add cost breakdown (stacked area for KM and Time costs)\n",
    "    if 'vehicle_km_cost' in company_ts.columns and 'vehicle_time_cost' in company_ts.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=company_ts['date'],\n",
    "                y=company_ts['vehicle_km_cost'],\n",
    "                mode='lines',\n",
    "                name='KM Cost',\n",
    "                line=dict(color='#ff7f0e', width=0),\n",
    "                fill='tozeroy',\n",
    "                legendgroup='cost_breakdown',\n",
    "                visible='legendonly'  # Hidden by default\n",
    "            ),\n",
    "            row=4, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=company_ts['date'],\n",
    "                y=company_ts['vehicle_time_cost'],\n",
    "                mode='lines',\n",
    "                name='Time Cost',\n",
    "                line=dict(color='#9467bd', width=0),\n",
    "                fill='tonexty',\n",
    "                legendgroup='cost_breakdown',\n",
    "                visible='legendonly'  # Hidden by default\n",
    "            ),\n",
    "            row=4, col=1\n",
    "        )\n",
    "\n",
    "# 5. Drivers (stacked)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=company_ts['date'],\n",
    "        y=company_ts['internal_drivers'],\n",
    "        mode='lines',\n",
    "        name='Internal Drivers',\n",
    "        line=dict(color='#2ca02c', width=0),\n",
    "        fill='tozeroy',\n",
    "        legendgroup='drivers'\n",
    "    ),\n",
    "    row=5, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=company_ts['date'],\n",
    "        y=company_ts['external_drivers'],\n",
    "        mode='lines',\n",
    "        name='External Drivers',\n",
    "        line=dict(color='#d62728', width=0),\n",
    "        fill='tonexty',\n",
    "        legendgroup='drivers'\n",
    "    ),\n",
    "    row=5, col=1\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_xaxes(title_text=\"Date\", row=5, col=1)\n",
    "fig.update_yaxes(title_text=\"Orders\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Kilometers\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Tours\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"CHF\", row=4, col=1)\n",
    "fig.update_yaxes(title_text=\"Driver Count\", row=5, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=1500,  # Increased from 1200 to accommodate 5 panels\n",
    "    title_text=\"Traveco Transport Metrics - Complete Time Series with Costs (2022-2024)\",\n",
    "    showlegend=True,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Save to results\n",
    "results_dir = Path('../results')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "fig.write_html(results_dir / 'time_series_overview.html')\n",
    "print(f\"\\n✓ Saved interactive chart to results/time_series_overview.html\")\n",
    "\n",
    "# Print KM efficiency summary if available\n",
    "if 'km_efficiency' in company_ts.columns:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"KM EFFICIENCY ANALYSIS (Actual / Billed)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    efficiency_stats = company_ts['km_efficiency'].describe()\n",
    "    print(f\"  Mean efficiency: {efficiency_stats['mean']:.2%}\")\n",
    "    print(f\"  Median efficiency: {efficiency_stats['50%']:.2%}\")\n",
    "    print(f\"  Min efficiency: {efficiency_stats['min']:.2%}\")\n",
    "    print(f\"  Max efficiency: {efficiency_stats['max']:.2%}\")\n",
    "    print(f\"\\n  Interpretation:\")\n",
    "    print(f\"    < 100%: More efficient (actual < billed)\")\n",
    "    print(f\"    = 100%: Perfect match\")\n",
    "    print(f\"    > 100%: Less efficient (actual > billed)\")\n",
    "    \n",
    "    # Count efficiency categories\n",
    "    excellent = (company_ts['km_efficiency'] < 0.9).sum()\n",
    "    good = ((company_ts['km_efficiency'] >= 0.9) & (company_ts['km_efficiency'] < 1.0)).sum()\n",
    "    acceptable = ((company_ts['km_efficiency'] >= 1.0) & (company_ts['km_efficiency'] < 1.1)).sum()\n",
    "    poor = (company_ts['km_efficiency'] >= 1.1).sum()\n",
    "    \n",
    "    print(f\"\\n  Efficiency Distribution:\")\n",
    "    print(f\"    Excellent (<90%): {excellent} months\")\n",
    "    print(f\"    Good (90-100%): {good} months\")\n",
    "    print(f\"    Acceptable (100-110%): {acceptable} months\")\n",
    "    print(f\"    Poor (>110%): {poor} months\")\n",
    "\n",
    "# Print cost summary if available\n",
    "if 'total_vehicle_cost' in company_ts.columns and 'revenue_total' in company_ts.columns:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINANCIAL SUMMARY (2022-2024)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    total_revenue = company_ts['revenue_total'].sum()\n",
    "    total_cost = company_ts['total_vehicle_cost'].sum()\n",
    "    total_profit = total_revenue - total_cost\n",
    "    avg_margin = (total_profit / total_revenue) * 100\n",
    "    \n",
    "    print(f\"  Total Revenue: CHF {total_revenue:,.2f}\")\n",
    "    print(f\"  Total Vehicle Cost: CHF {total_cost:,.2f}\")\n",
    "    print(f\"  Total Profit: CHF {total_profit:,.2f}\")\n",
    "    print(f\"  Average Profit Margin: {avg_margin:.2f}%\")\n",
    "    \n",
    "    if 'vehicle_km_cost' in company_ts.columns and 'vehicle_time_cost' in company_ts.columns:\n",
    "        km_cost_total = company_ts['vehicle_km_cost'].sum()\n",
    "        time_cost_total = company_ts['vehicle_time_cost'].sum()\n",
    "        km_pct = (km_cost_total / total_cost) * 100\n",
    "        time_pct = (time_cost_total / total_cost) * 100\n",
    "        \n",
    "        print(f\"\\n  Cost Breakdown:\")\n",
    "        print(f\"    KM-based cost: CHF {km_cost_total:,.2f} ({km_pct:.1f}%)\")\n",
    "        print(f\"    Time-based cost: CHF {time_cost_total:,.2f} ({time_pct:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Seasonality Analysis\n",
    "\n",
    "Identify seasonal patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract month from date\n",
    "company_ts['month'] = company_ts['date'].dt.month\n",
    "company_ts['year'] = company_ts['date'].dt.year\n",
    "\n",
    "# Calculate monthly averages\n",
    "# Use total_km_billed if available, otherwise use total_km\n",
    "km_col = 'total_km_billed' if 'total_km_billed' in company_ts.columns else 'total_km'\n",
    "monthly_avg = company_ts.groupby('month').agg({\n",
    "    'total_orders': 'mean',\n",
    "    km_col: 'mean',\n",
    "    'total_drivers': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Month names\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "monthly_avg['month_name'] = monthly_avg['month'].apply(lambda x: month_names[x-1])\n",
    "\n",
    "# Create seasonality chart\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=monthly_avg['month_name'],\n",
    "        y=monthly_avg['total_orders'],\n",
    "        name='Avg Orders',\n",
    "        marker_color='#1f77b4'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Seasonal Pattern - Average Orders by Month (2022-2025)\",\n",
    "    xaxis_title=\"Month\",\n",
    "    yaxis_title=\"Average Monthly Orders\",\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Identify peak and low months\n",
    "peak_month = monthly_avg.loc[monthly_avg['total_orders'].idxmax(), 'month_name']\n",
    "low_month = monthly_avg.loc[monthly_avg['total_orders'].idxmin(), 'month_name']\n",
    "\n",
    "print(f\"\\nSeasonality Insights:\")\n",
    "print(f\"  Peak month: {peak_month} ({monthly_avg['total_orders'].max():.0f} orders)\")\n",
    "print(f\"  Low month: {low_month} ({monthly_avg['total_orders'].min():.0f} orders)\")\n",
    "print(f\"  Seasonal variation: {(monthly_avg['total_orders'].max() / monthly_avg['total_orders'].min() - 1) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Save Final Datasets\n",
    "\n",
    "Save both Betriebszentralen-level and company-level time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path('../data/processed')\n",
    "\n",
    "# 1. Save Betriebszentralen-level time series\n",
    "bz_output = output_dir / 'monthly_aggregated_full_bz.csv'\n",
    "print(f\"Saving Betriebszentralen-level time series to {bz_output}...\")\n",
    "df_full_ts.to_csv(bz_output, index=False)\n",
    "print(f\"✓ Saved {len(df_full_ts):,} records\")\n",
    "\n",
    "# 2. Save company-level time series\n",
    "company_output = output_dir / 'monthly_aggregated_full_company.csv'\n",
    "print(f\"\\nSaving company-level time series to {company_output}...\")\n",
    "company_ts.to_csv(company_output, index=False)\n",
    "print(f\"✓ Saved {len(company_ts)} months\")\n",
    "\n",
    "# 3. Save as Parquet for faster loading in forecasting notebooks\n",
    "company_ts.to_parquet(output_dir / 'monthly_aggregated_full_company.parquet', index=False)\n",
    "print(f\"✓ Saved Parquet version\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TIME SERIES AGGREGATION COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nDatasets ready for forecasting:\")\n",
    "print(f\"  1. Betriebszentralen-level: {len(df_full_ts):,} month-entity records\")\n",
    "print(f\"  2. Company-level: {len(company_ts)} monthly records\")\n",
    "print(f\"  3. Time range: {company_ts['date'].min()} to {company_ts['date'].max()}\")\n",
    "print(f\"  4. Total months: {len(company_ts)}\")\n",
    "print(f\"\\nNext step: Proceed to forecasting notebooks (09-15)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Summary Statistics\n",
    "\n",
    "Final overview of the complete time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Complete Time Series Summary\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. Coverage:\")\n",
    "print(f\"   Start date: {company_ts['date'].min()}\")\n",
    "print(f\"   End date: {company_ts['date'].max()}\")\n",
    "print(f\"   Total months: {len(company_ts)}\")\n",
    "print(f\"   Years covered: {sorted(company_ts['year'].unique())}\")\n",
    "\n",
    "# Use fallback for km column\n",
    "km_col = 'total_km_billed' if 'total_km_billed' in company_ts.columns else 'total_km'\n",
    "\n",
    "print(f\"\\n2. Totals (across all months):\")\n",
    "print(f\"   Total orders: {company_ts['total_orders'].sum():,.0f}\")\n",
    "print(f\"   Total KM ({km_col}): {company_ts[km_col].sum():,.0f}\")\n",
    "print(f\"   Avg KM/order: {(company_ts[km_col].sum() / company_ts['total_orders'].sum()):.1f}\")\n",
    "\n",
    "print(f\"\\n3. Monthly Averages:\")\n",
    "print(f\"   Avg orders/month: {company_ts['total_orders'].mean():,.0f}\")\n",
    "print(f\"   Avg KM/month: {company_ts[km_col].mean():,.0f}\")\n",
    "print(f\"   Avg drivers/month: {company_ts['total_drivers'].mean():.0f}\")\n",
    "\n",
    "print(f\"\\n4. Betriebszentralen:\")\n",
    "print(f\"   Total entities: {df_full_ts['betriebszentrale'].nunique()}\")\n",
    "print(f\"   List: {sorted(df_full_ts['betriebszentrale'].unique())}\")\n",
    "\n",
    "print(f\"\\n5. Data Quality:\")\n",
    "print(f\"   Missing values: {company_ts.isna().sum().sum()}\")\n",
    "print(f\"   Duplicate months: {company_ts['year_month'].duplicated().sum()}\")\n",
    "\n",
    "print(f\"\\n✓ Dataset is ready for time series forecasting!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
