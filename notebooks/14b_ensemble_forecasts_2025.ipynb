{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Forecasting for 2025\n",
    "\n",
    "This notebook creates ensemble forecasts by combining predictions from multiple models.\n",
    "\n",
    "## Ensemble Methods\n",
    "1. **Simple Average**: Equal weight to all models\n",
    "2. **Weighted by Inverse MAPE**: Better models get higher weight\n",
    "3. **Top-2 Average**: Use only the best 2 models per metric\n",
    "4. **Hybrid ML+Human**: Blend machine learning with traditional forecasts\n",
    "\n",
    "## Available Model Forecasts\n",
    "- **CatBoost**: Gradient boosting with categorical features (WORKS - monthly variation)\n",
    "- **Seasonal Naive**: Previous year's value (simple baseline)\n",
    "- **Prophet**: Time series decomposition\n",
    "- **SARIMAX**: Statistical ARIMA model\n",
    "- **Human Method**: 2024 average (total ÷ 12)\n",
    "\n",
    "**Note**: LightGBM/XGBoost excluded due to flat forecasts (insufficient training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Model Forecasts and Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2025 forecasts from each model\n",
    "data_dir = Path('../data/processed')\n",
    "\n",
    "# CatBoost (ML champion)\n",
    "catboost_2025 = pd.read_csv(data_dir / 'catboost_forecast_2025.csv')\n",
    "catboost_2025['date'] = pd.to_datetime(catboost_2025['date'])\n",
    "\n",
    "# Load historical data for baselines\n",
    "df_hist = pd.read_csv(data_dir / 'monthly_aggregated_full_company.csv')\n",
    "df_hist['date'] = pd.to_datetime(df_hist['date'])\n",
    "\n",
    "print(f\"✓ Loaded CatBoost forecasts: {len(catboost_2025)} months\")\n",
    "print(f\"✓ Loaded historical data: {len(df_hist)} months ({df_hist['date'].min()} to {df_hist['date'].max()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation performance metrics\n",
    "catboost_metrics = pd.read_csv(data_dir / 'catboost_metrics.csv')\n",
    "baseline_metrics = pd.read_csv(data_dir / 'baseline_metrics.csv')\n",
    "prophet_metrics = pd.read_csv(data_dir / 'prophet_metrics.csv')\n",
    "sarimax_metrics = pd.read_csv(data_dir / 'sarimax_metrics.csv')\n",
    "\n",
    "print(\"\\n✓ Loaded performance metrics for all models\")\n",
    "print(f\"  - CatBoost: {len(catboost_metrics)} metrics\")\n",
    "print(f\"  - Baseline: {len(baseline_metrics)} metrics\")\n",
    "print(f\"  - Prophet: {len(prophet_metrics)} metrics\")\n",
    "print(f\"  - SARIMAX: {len(sarimax_metrics)} metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Generate Baseline 2025 Forecasts\n",
    "\n",
    "Create simple baseline forecasts for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target metrics (all 10)\n",
    "target_metrics = [\n",
    "    'total_orders',\n",
    "    'total_km_billed',\n",
    "    'total_km_actual',\n",
    "    'total_tours',\n",
    "    'total_drivers',\n",
    "    'revenue_total',\n",
    "    'external_drivers',\n",
    "    'vehicle_km_cost',\n",
    "    'vehicle_time_cost',\n",
    "    'total_vehicle_cost'\n",
    "]\n",
    "\n",
    "# Create 2025 date range\n",
    "dates_2025 = pd.date_range(start='2025-01-01', end='2025-12-01', freq='MS')\n",
    "\n",
    "# Initialize forecast dataframes\n",
    "seasonal_naive_2025 = pd.DataFrame({'date': dates_2025})\n",
    "human_method_2025 = pd.DataFrame({'date': dates_2025})\n",
    "\n",
    "print(\"Generating baseline forecasts...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for metric in target_metrics:\n",
    "    # Seasonal Naive: Use 2024 values\n",
    "    values_2024 = df_hist[df_hist['date'].dt.year == 2024][metric].values\n",
    "    if len(values_2024) == 12:\n",
    "        seasonal_naive_2025[metric] = values_2024\n",
    "    else:\n",
    "        print(f\"  ⚠️  Warning: {metric} has {len(values_2024)} months in 2024, expected 12\")\n",
    "        seasonal_naive_2025[metric] = values_2024.mean()  # Fallback to mean\n",
    "    \n",
    "    # Human Method: 2024 total ÷ 12\n",
    "    total_2024 = df_hist[df_hist['date'].dt.year == 2024][metric].sum()\n",
    "    human_method_2025[metric] = total_2024 / 12\n",
    "    \n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"  Seasonal Naive: {seasonal_naive_2025[metric].min():.0f} - {seasonal_naive_2025[metric].max():.0f}\")\n",
    "    print(f\"  Human Method: {human_method_2025[metric].iloc[0]:.0f} (constant)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Baseline forecasts generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Create Model Performance Summary\n",
    "\n",
    "Identify best model per metric based on validation MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all metrics\n",
    "all_metrics = pd.concat([\n",
    "    catboost_metrics,\n",
    "    baseline_metrics[baseline_metrics['model'] == 'Seasonal Naive'],  # Best baseline\n",
    "    prophet_metrics,\n",
    "    sarimax_metrics\n",
    "], ignore_index=True)\n",
    "\n",
    "# Find best model per metric\n",
    "best_models = all_metrics.loc[all_metrics.groupby('metric')['MAPE'].idxmin()]\n",
    "\n",
    "print(\"Best Model per Metric (by MAPE):\")\n",
    "print(\"=\"*80)\n",
    "for _, row in best_models.iterrows():\n",
    "    print(f\"{row['metric']:25s} → {row['model']:15s} (MAPE: {row['MAPE']:.2f}%)\")\n",
    "\n",
    "# Count wins\n",
    "model_wins = best_models['model'].value_counts()\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Model Win Count:\")\n",
    "print(\"=\"*80)\n",
    "for model, count in model_wins.items():\n",
    "    print(f\"  {model}: {count}/10 metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Ensemble Method 1 - Weighted by Inverse MAPE\n",
    "\n",
    "Weight each model's forecast by its inverse MAPE (better models get higher weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ensemble forecast\n",
    "ensemble_weighted_2025 = pd.DataFrame({'date': dates_2025})\n",
    "\n",
    "print(\"Creating Weighted Ensemble (Inverse MAPE)...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for metric in target_metrics:\n",
    "    print(f\"\\n{metric}:\")\n",
    "    \n",
    "    # Get MAPE for each model\n",
    "    metric_perf = all_metrics[all_metrics['metric'] == metric].copy()\n",
    "    \n",
    "    # Calculate inverse MAPE weights (lower MAPE = higher weight)\n",
    "    metric_perf['inv_mape'] = 1 / (metric_perf['MAPE'] + 0.01)  # Add small value to avoid division by zero\n",
    "    metric_perf['weight'] = metric_perf['inv_mape'] / metric_perf['inv_mape'].sum()\n",
    "    \n",
    "    # Get forecasts for this metric\n",
    "    forecasts = {\n",
    "        'CatBoost': catboost_2025[metric].values,\n",
    "        'Seasonal Naive': seasonal_naive_2025[metric].values,\n",
    "        'Human Method': human_method_2025[metric].values\n",
    "    }\n",
    "    \n",
    "    # Calculate weighted average\n",
    "    weighted_forecast = np.zeros(12)\n",
    "    \n",
    "    for _, row in metric_perf.iterrows():\n",
    "        model_name = row['model']\n",
    "        weight = row['weight']\n",
    "        \n",
    "        if model_name in forecasts:\n",
    "            weighted_forecast += forecasts[model_name] * weight\n",
    "            print(f\"  {model_name:15s}: weight={weight:.3f}, MAPE={row['MAPE']:.2f}%\")\n",
    "    \n",
    "    ensemble_weighted_2025[metric] = weighted_forecast\n",
    "    print(f\"  → Ensemble: {weighted_forecast.min():.0f} - {weighted_forecast.max():.0f} (variation: {((weighted_forecast.max()/weighted_forecast.min()-1)*100):.1f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ Weighted ensemble created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Ensemble Method 2 - Best Model per Metric\n",
    "\n",
    "Use the single best model for each metric (no averaging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize best-model ensemble\n",
    "ensemble_best_2025 = pd.DataFrame({'date': dates_2025})\n",
    "\n",
    "print(\"Creating Best-Model Ensemble...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for metric in target_metrics:\n",
    "    # Get best model for this metric\n",
    "    best_row = best_models[best_models['metric'] == metric].iloc[0]\n",
    "    best_model = best_row['model']\n",
    "    best_mape = best_row['MAPE']\n",
    "    \n",
    "    # Select forecast from best model\n",
    "    if best_model == 'CatBoost':\n",
    "        ensemble_best_2025[metric] = catboost_2025[metric].values\n",
    "    elif best_model == 'Seasonal Naive':\n",
    "        ensemble_best_2025[metric] = seasonal_naive_2025[metric].values\n",
    "    elif best_model == 'Human Method':\n",
    "        ensemble_best_2025[metric] = human_method_2025[metric].values\n",
    "    else:\n",
    "        # Fallback to CatBoost\n",
    "        print(f\"  ⚠️  {metric}: {best_model} not available, using CatBoost\")\n",
    "        ensemble_best_2025[metric] = catboost_2025[metric].values\n",
    "    \n",
    "    print(f\"{metric:25s} → {best_model:15s} (MAPE: {best_mape:.2f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ Best-model ensemble created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Ensemble Method 3 - Hybrid ML+Human\n",
    "\n",
    "Blend CatBoost (ML) with Human Method (60% ML / 40% Human)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hybrid ensemble\n",
    "ensemble_hybrid_2025 = pd.DataFrame({'date': dates_2025})\n",
    "\n",
    "ml_weight = 0.6\n",
    "human_weight = 0.4\n",
    "\n",
    "print(f\"Creating Hybrid Ensemble ({ml_weight*100:.0f}% ML / {human_weight*100:.0f}% Human)...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for metric in target_metrics:\n",
    "    # Blend CatBoost and Human Method\n",
    "    ensemble_hybrid_2025[metric] = (\n",
    "        catboost_2025[metric].values * ml_weight +\n",
    "        human_method_2025[metric].values * human_weight\n",
    "    )\n",
    "    \n",
    "    print(f\"{metric:25s}: {ensemble_hybrid_2025[metric].min():.0f} - {ensemble_hybrid_2025[metric].max():.0f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ Hybrid ensemble created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Save Ensemble Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all ensemble forecasts\n",
    "output_dir = Path('../data/processed')\n",
    "\n",
    "ensemble_weighted_2025.to_csv(output_dir / 'ensemble_weighted_2025.csv', index=False)\n",
    "ensemble_best_2025.to_csv(output_dir / 'ensemble_best_model_2025.csv', index=False)\n",
    "ensemble_hybrid_2025.to_csv(output_dir / 'ensemble_hybrid_2025.csv', index=False)\n",
    "\n",
    "# Also save baselines for reference\n",
    "seasonal_naive_2025.to_csv(output_dir / 'seasonal_naive_2025.csv', index=False)\n",
    "human_method_2025.to_csv(output_dir / 'human_method_2025.csv', index=False)\n",
    "\n",
    "print(\"Saved ensemble forecasts:\")\n",
    "print(\"  ✓ ensemble_weighted_2025.csv (inverse MAPE weights)\")\n",
    "print(\"  ✓ ensemble_best_model_2025.csv (best model per metric)\")\n",
    "print(\"  ✓ ensemble_hybrid_2025.csv (60% ML / 40% Human)\")\n",
    "print(\"  ✓ seasonal_naive_2025.csv (baseline)\")\n",
    "print(\"  ✓ human_method_2025.csv (baseline)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Visualize Ensemble Comparison\n",
    "\n",
    "Compare all ensemble methods for key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize for key metrics\n",
    "key_metrics = ['total_orders', 'revenue_total', 'total_drivers', 'total_vehicle_cost']\n",
    "\n",
    "for metric in key_metrics:\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add each ensemble method\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=ensemble_weighted_2025['date'],\n",
    "        y=ensemble_weighted_2025[metric],\n",
    "        mode='lines+markers',\n",
    "        name='Weighted (Inverse MAPE)',\n",
    "        line=dict(width=3)\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=ensemble_best_2025['date'],\n",
    "        y=ensemble_best_2025[metric],\n",
    "        mode='lines+markers',\n",
    "        name='Best Model',\n",
    "        line=dict(width=2, dash='dash')\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=ensemble_hybrid_2025['date'],\n",
    "        y=ensemble_hybrid_2025[metric],\n",
    "        mode='lines+markers',\n",
    "        name='Hybrid (60% ML / 40% Human)',\n",
    "        line=dict(width=2, dash='dot')\n",
    "    ))\n",
    "    \n",
    "    # Add individual models for reference\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=catboost_2025['date'],\n",
    "        y=catboost_2025[metric],\n",
    "        mode='lines',\n",
    "        name='CatBoost (ML)',\n",
    "        line=dict(width=1, color='lightgray'),\n",
    "        opacity=0.5\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=seasonal_naive_2025['date'],\n",
    "        y=seasonal_naive_2025[metric],\n",
    "        mode='lines',\n",
    "        name='Seasonal Naive',\n",
    "        line=dict(width=1, color='lightblue'),\n",
    "        opacity=0.5\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"2025 Ensemble Forecasts - {metric.replace('_', ' ').title()}\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=metric.replace('_', ' ').title(),\n",
    "        height=600,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Save\n",
    "    results_dir = Path('../results')\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    fig.write_html(results_dir / f'ensemble_comparison_{metric}.html')\n",
    "    print(f\"✓ Saved: results/ensemble_comparison_{metric}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Ensemble Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_data = []\n",
    "\n",
    "for metric in target_metrics:\n",
    "    summary_data.append({\n",
    "        'Metric': metric,\n",
    "        'CatBoost Min': catboost_2025[metric].min(),\n",
    "        'CatBoost Max': catboost_2025[metric].max(),\n",
    "        'CatBoost Var%': ((catboost_2025[metric].max() / catboost_2025[metric].min() - 1) * 100),\n",
    "        'Weighted Min': ensemble_weighted_2025[metric].min(),\n",
    "        'Weighted Max': ensemble_weighted_2025[metric].max(),\n",
    "        'Weighted Var%': ((ensemble_weighted_2025[metric].max() / ensemble_weighted_2025[metric].min() - 1) * 100),\n",
    "        'Best Model': best_models[best_models['metric'] == metric]['model'].iloc[0],\n",
    "        'Best MAPE': best_models[best_models['metric'] == metric]['MAPE'].iloc[0]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\nEnsemble Forecast Summary:\")\n",
    "print(\"=\"*120)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(output_dir / 'ensemble_summary.csv', index=False)\n",
    "print(\"\\n✓ Saved: data/processed/ensemble_summary.csv\")\n",
    "\n",
    "print(f\"\\n{'='*120}\")\n",
    "print(\"ENSEMBLE FORECASTING COMPLETE!\")\n",
    "print(\"=\"*120)\n",
    "print(\"\\nThree ensemble methods created:\")\n",
    "print(\"  1. Weighted by Inverse MAPE - Better models get higher weight\")\n",
    "print(\"  2. Best Model per Metric - Use single best performer\")\n",
    "print(\"  3. Hybrid ML+Human - Blend CatBoost (60%) with human judgment (40%)\")\n",
    "print(\"\\nNext: Run Notebook 18 to validate all forecasts against actual 2025 data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
