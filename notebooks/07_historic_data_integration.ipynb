{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historic Data Integration (2022-2024)\n",
    "\n",
    "This notebook consolidates 36 months of historic transport data (January 2022 - December 2024) into a unified dataset.\n",
    "\n",
    "## Objectives\n",
    "1. Load 36 monthly files from 2022-2024\n",
    "2. Standardize column names across years\n",
    "3. Apply filtering rules (exclude Lager and B&T pickup orders)\n",
    "4. Apply feature engineering (Betriebszentralen, Sparten, temporal features)\n",
    "5. Save consolidated dataset for time series forecasting\n",
    "\n",
    "## Data Coverage\n",
    "- **2022**: 1,700,564 records (12 months)\n",
    "- **2023**: 1,685,937 records (12 months)\n",
    "- **2024**: 1,684,729 records (12 months)\n",
    "- **Total**: 5,071,230 records before filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from utils.traveco_utils import TravecomDataCleaner, TravecomFeatureEngine\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Config loaded from: /Users/kk/dev/customer_traveco/config/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Initialize config once for all sections\n",
    "from pathlib import Path\n",
    "from utils.traveco_utils import ConfigLoader\n",
    "\n",
    "config_path = Path('../config/config.yaml')\n",
    "config = ConfigLoader(config_path=str(config_path))\n",
    "print(f\"✓ Config loaded from: {config_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 4,935,622 records!\n"
     ]
    }
   ],
   "source": [
    "# Quick reload\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "raw_pickle = Path('../data/processed/historic_orders_2022_2024_RAW.pkl')\n",
    "df_historic = pd.read_pickle(raw_pickle)\n",
    "print(f\"✓ Loaded {len(df_historic):,} records!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Multi-Year Data Loader\n",
    "\n",
    "Load all 36 monthly files from 2022-2024 and consolidate into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_historic_data(years=[2022, 2023, 2024], base_path='../data/raw'):\n",
    "    \"\"\"\n",
    "    Load and consolidate multi-year historic data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    years : list\n",
    "        Years to load (default: [2022, 2023, 2024])\n",
    "    base_path : str\n",
    "        Base path to raw data folders\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Consolidated dataframe with all historic data\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    file_count = 0\n",
    "    \n",
    "    for year in years:\n",
    "        year_path = Path(base_path) / str(year)\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Loading {year} data...\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Get all order analysis files\n",
    "        files = sorted([f for f in os.listdir(year_path) \n",
    "                       if 'QS Auftragsanalyse' in f and f.endswith('.xlsx')])\n",
    "        \n",
    "        print(f\"Found {len(files)} monthly files for {year}\")\n",
    "        \n",
    "        year_data = []\n",
    "        for f in tqdm(files, desc=f\"{year}\"):\n",
    "            filepath = year_path / f\n",
    "            \n",
    "            try:\n",
    "                # Load Excel file\n",
    "                df = pd.read_excel(filepath)\n",
    "                \n",
    "                # Add source tracking\n",
    "                df['source_file'] = f\n",
    "                df['source_year'] = year\n",
    "                \n",
    "                year_data.append(df)\n",
    "                file_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n  ❌ ERROR loading {f}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Concatenate year data\n",
    "        if year_data:\n",
    "            year_df = pd.concat(year_data, ignore_index=True)\n",
    "            print(f\"  ✓ {year}: {len(year_df):,} records loaded\")\n",
    "            all_data.append(year_df)\n",
    "    \n",
    "    # Consolidate all years\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Consolidating all years...\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df_all = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n✓ Total files loaded: {file_count}\")\n",
    "    print(f\"✓ Total records: {len(df_all):,}\")\n",
    "    print(f\"✓ Columns: {len(df_all.columns)}\")\n",
    "    print(f\"✓ Memory usage: {df_all.memory_usage(deep=True).sum() / 1024**3:.2f} GB\")\n",
    "    \n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing processed data. Loading from Parquet...\n",
      "✓ Loaded 4,937,096 records in seconds!\n",
      "✓ Memory: 29.59 GB\n",
      "\n",
      "Dataset shape: (4937096, 121)\n",
      "Date range: 2022-01-01 00:00:00 to 2024-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Quick load: Skip Excel loading if processed file exists\n",
    "processed_path = Path('../data/processed/historic_orders_2022_2024.parquet')\n",
    "\n",
    "if processed_path.exists():\n",
    "    print(\"Found existing processed data. Loading from Parquet...\")\n",
    "    df_historic = pd.read_parquet(processed_path)\n",
    "    print(f\"✓ Loaded {len(df_historic):,} records in seconds!\")\n",
    "    print(f\"✓ Memory: {df_historic.memory_usage(deep=True).sum() / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Skip to Section 6 (validation) or Section 8 (preview)\n",
    "    SKIP_LOADING = True\n",
    "else:\n",
    "    print(\"Processed data not found. Will load from raw Excel files...\")\n",
    "    SKIP_LOADING = False\n",
    "\n",
    "\n",
    "if not SKIP_LOADING:\n",
    "    # Load all historic data (30 minutes)\n",
    "    df_historic = load_historic_data()\n",
    "\n",
    "print(f\"\\nDataset shape: {df_historic.shape}\")\n",
    "print(f\"Date range: {df_historic['Datum.Tour'].min()} to {df_historic['Datum.Tour'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Column Name Standardization\n",
    "\n",
    "Handle minor schema differences between years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final column count: 121\n"
     ]
    }
   ],
   "source": [
    "def standardize_columns(df):\n",
    "    \"\"\"\n",
    "    Standardize column names to match 2025 schema.\n",
    "    \n",
    "    Changes:\n",
    "    - RKdNr. → RKdNr (remove trailing dot)\n",
    "    - Drop 'B&T Lt' (only in 2022-2024, not in 2025)\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Rename columns\n",
    "    rename_map = {}\n",
    "    \n",
    "    if 'RKdNr.' in df_clean.columns:\n",
    "        rename_map['RKdNr.'] = 'RKdNr'\n",
    "    \n",
    "    if rename_map:\n",
    "        df_clean = df_clean.rename(columns=rename_map)\n",
    "        print(f\"✓ Renamed columns: {list(rename_map.keys())}\")\n",
    "    \n",
    "    # Drop columns not in 2025\n",
    "    drop_cols = [col for col in ['B&T Lt'] if col in df_clean.columns]\n",
    "    if drop_cols:\n",
    "        df_clean = df_clean.drop(columns=drop_cols)\n",
    "        print(f\"✓ Dropped columns: {drop_cols}\")\n",
    "    \n",
    "    print(f\"\\nFinal column count: {len(df_clean.columns)}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply standardization\n",
    "df_historic = standardize_columns(df_historic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING RAW LOADED DATA (using Pickle)...\n",
      "================================================================================\n",
      "Saving to ../data/processed/historic_orders_2022_2024_RAW.pkl...\n",
      "✓ Saved Pickle: 3374.9 MB\n",
      "\n",
      "================================================================================\n",
      "RAW DATA SAVED!\n",
      "Reload with: df_historic = pd.read_pickle('../data/processed/historic_orders_2022_2024_RAW.pkl')\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# EMERGENCY SAVE using Pickle (handles mixed types)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RAW LOADED DATA (using Pickle)...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Use Pickle - it handles mixed types without issues\n",
    "raw_pickle = output_dir / 'historic_orders_2022_2024_RAW.pkl'\n",
    "print(f\"Saving to {raw_pickle}...\")\n",
    "df_historic.to_pickle(raw_pickle)\n",
    "file_size = raw_pickle.stat().st_size / 1024**2\n",
    "print(f\"✓ Saved Pickle: {file_size:.1f} MB\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RAW DATA SAVED!\")\n",
    "print(f\"Reload with: df_historic = pd.read_pickle('{raw_pickle}')\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Data Validation\n",
    "\n",
    "Verify data quality before filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Checks:\n",
      "================================================================================\n",
      "\n",
      "1. Date Range:\n",
      "   Min: 2022-01-01 00:00:00\n",
      "   Max: 2024-12-31 00:00:00\n",
      "   Missing: 0\n",
      "\n",
      "2. Key Column Completeness:\n",
      "   Nummer.Auftraggeber: 6 missing (0.00%)\n",
      "   Lieferart 2.0: 0 missing (0.00%)\n",
      "   System_id.Auftrag: 0 missing (0.00%)\n",
      "   Nummer.Spedition: 0 missing (0.00%)\n",
      "   Distanz_BE.Auftrag: 0 missing (0.00%)\n",
      "\n",
      "3. Monthly Distribution:\n",
      "   Total months: 36\n",
      "   Avg records/month: 137,142\n",
      "   Min: 126,066 (2023-04)\n",
      "   Max: 154,679 (2022-03)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Quality Checks:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check date column\n",
    "print(f\"\\n1. Date Range:\")\n",
    "print(f\"   Min: {df_historic['Datum.Tour'].min()}\")\n",
    "print(f\"   Max: {df_historic['Datum.Tour'].max()}\")\n",
    "print(f\"   Missing: {df_historic['Datum.Tour'].isna().sum():,}\")\n",
    "\n",
    "# Check key columns\n",
    "key_cols = ['Nummer.Auftraggeber', 'Lieferart 2.0', 'System_id.Auftrag', \n",
    "            'Nummer.Spedition', 'Distanz_BE.Auftrag']\n",
    "\n",
    "print(f\"\\n2. Key Column Completeness:\")\n",
    "for col in key_cols:\n",
    "    if col in df_historic.columns:\n",
    "        missing = df_historic[col].isna().sum()\n",
    "        pct = (missing / len(df_historic)) * 100\n",
    "        print(f\"   {col}: {missing:,} missing ({pct:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"   ❌ {col}: NOT FOUND\")\n",
    "\n",
    "# Monthly distribution\n",
    "print(f\"\\n3. Monthly Distribution:\")\n",
    "df_historic['year_month'] = pd.to_datetime(df_historic['Datum.Tour']).dt.to_period('M')\n",
    "monthly_counts = df_historic['year_month'].value_counts().sort_index()\n",
    "print(f\"   Total months: {len(monthly_counts)}\")\n",
    "print(f\"   Avg records/month: {monthly_counts.mean():,.0f}\")\n",
    "print(f\"   Min: {monthly_counts.min():,} ({monthly_counts.idxmin()})\")\n",
    "print(f\"   Max: {monthly_counts.max():,} ({monthly_counts.idxmax()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Apply Filtering Rules\n",
    "\n",
    "Exclude irrelevant orders:\n",
    "1. **Lager Aufträge** (warehouse orders)\n",
    "2. **B&T Abholaufträge** (B&T internal pickups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Filtering Rules...\n",
      "================================================================================\n",
      "\n",
      "✂️  Applying Filtering Rules (Christian's Feedback - Oct 2025):\n",
      "   Starting orders: 4,937,096\n",
      "   ℹ️  No B&T pickup orders found (already filtered or not present)\n",
      "\n",
      "   ✓ No orders filtered (all 4,937,096 orders passed filtering rules)\n",
      "\n",
      "Filtering Summary:\n",
      "  Original records: 4,937,096\n",
      "  Filtered records: 4,937,096\n",
      "  Excluded: 0 (0.00%)\n",
      "  Retention rate: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying Filtering Rules...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize cleaner with correct config path\n",
    "from pathlib import Path\n",
    "config_path = Path('../config/config.yaml')  # Relative to notebooks/ directory\n",
    "\n",
    "# Import ConfigLoader if not already imported\n",
    "from utils.traveco_utils import ConfigLoader, TravecomDataCleaner\n",
    "\n",
    "# Initialize cleaner with explicit config path\n",
    "config = ConfigLoader(config_path=str(config_path))\n",
    "cleaner = TravecomDataCleaner(config=config)\n",
    "\n",
    "# Apply filters\n",
    "df_filtered = cleaner.apply_filtering_rules(df_historic)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nFiltering Summary:\")\n",
    "print(f\"  Original records: {len(df_historic):,}\")\n",
    "print(f\"  Filtered records: {len(df_filtered):,}\")\n",
    "print(f\"  Excluded: {len(df_historic) - len(df_filtered):,} ({((len(df_historic) - len(df_filtered)) / len(df_historic)) * 100:.2f}%)\")\n",
    "print(f\"  Retention rate: {(len(df_filtered) / len(df_historic)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Feature Engineering\n",
    "\n",
    "Apply the same feature engineering pipeline used for June 2025 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Feature Engineering...\n",
      "================================================================================\n",
      "\n",
      "1. Extracting temporal features...\n",
      "   ✓ Added: year, month, quarter, week, day_of_year, weekday, is_weekend\n",
      "\n",
      "2. Identifying carrier types...\n",
      "   ✓ Carrier types:\n",
      "carrier_type\n",
      "internal    3726353\n",
      "external    1123805\n",
      "unknown       86938\n",
      "Name: count, dtype: int64\n",
      "\n",
      "3. Classifying order types...\n",
      "   ✓ Order types:\n",
      "order_type\n",
      "Delivery            3448760\n",
      "Leergut              905051\n",
      "Pickup/Multi-leg     570874\n",
      "Retoure/Abholung      12411\n",
      "Name: count, dtype: int64\n",
      "\n",
      "4. Mapping Betriebszentralen (14 dispatch centers)...\n",
      "   ✓ Betriebszentralen mapped successfully\n",
      "   ✓ Distribution:\n",
      "      BZ Oberbipp: 1,450,642\n",
      "      BZ Winterthur: 1,099,573\n",
      "      BZ Sursee: 1,090,085\n",
      "      BZ Landquart: 654,664\n",
      "      BZ Herzogenbuchsee: 253,542\n",
      "      B&T Winterthur: 190,812\n",
      "      B&T Puidoux: 94,024\n",
      "      BZ Sierre: 48,925\n",
      "      BZ Puidoux: 24,740\n",
      "      B&T Landquart: 17,084\n",
      "      BZ Intermodal / Rail: 12,999\n",
      "      Unknown: 6\n",
      "\n",
      "5. Mapping Sparten (customer divisions)...\n",
      "   ✓ Loaded Sparten from 2024 Sparten.xlsx\n",
      "   ✓ Sparten mapped successfully\n",
      "   ✓ Top 10 Sparten:\n",
      "      8401: 3,428,868\n",
      "      3360: 410,668\n",
      "      3293: 301,207\n",
      "      6210: 152,804\n",
      "      3008: 119,296\n",
      "      8408: 69,513\n",
      "      8560: 64,218\n",
      "      6340: 51,757\n",
      "      3270: 44,005\n",
      "      3421: 24,800\n",
      "\n",
      "================================================================================\n",
      "Feature Engineering Complete!\n",
      "  Final columns: 123\n",
      "  Final records: 4,940,044\n",
      "  Memory usage: 30.46 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying Feature Engineering...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Start with filtered data\n",
    "df_featured = df_filtered.copy()\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Temporal Features\n",
    "# ==============================================================================\n",
    "print(\"\\n1. Extracting temporal features...\")\n",
    "df_featured['Datum.Tour'] = pd.to_datetime(df_featured['Datum.Tour'])\n",
    "\n",
    "df_featured['year'] = df_featured['Datum.Tour'].dt.year\n",
    "df_featured['month'] = df_featured['Datum.Tour'].dt.month\n",
    "df_featured['quarter'] = df_featured['Datum.Tour'].dt.quarter\n",
    "df_featured['week'] = df_featured['Datum.Tour'].dt.isocalendar().week\n",
    "df_featured['day_of_year'] = df_featured['Datum.Tour'].dt.dayofyear\n",
    "df_featured['weekday'] = df_featured['Datum.Tour'].dt.weekday\n",
    "df_featured['is_weekend'] = df_featured['weekday'].isin([5, 6]).astype(int)\n",
    "\n",
    "print(f\"   ✓ Added: year, month, quarter, week, day_of_year, weekday, is_weekend\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Carrier Type Classification\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Identifying carrier types...\")\n",
    "\n",
    "INTERNAL_MAX = 8889\n",
    "EXTERNAL_MIN = 9000\n",
    "\n",
    "def classify_carrier(carrier_num):\n",
    "    \"\"\"Classify carrier as internal, external, or unknown\"\"\"\n",
    "    if pd.isna(carrier_num):\n",
    "        return 'unknown'\n",
    "    \n",
    "    try:\n",
    "        carrier_num = float(str(carrier_num).replace('-', '').strip())\n",
    "    except (ValueError, AttributeError):\n",
    "        return 'unknown'\n",
    "    \n",
    "    if carrier_num <= INTERNAL_MAX:\n",
    "        return 'internal'\n",
    "    elif carrier_num >= EXTERNAL_MIN:\n",
    "        return 'external'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "df_featured['carrier_type'] = df_featured['Nummer.Spedition'].apply(classify_carrier)\n",
    "print(f\"   ✓ Carrier types:\")\n",
    "print(df_featured['carrier_type'].value_counts())\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Order Type Classification\n",
    "# ==============================================================================\n",
    "print(\"\\n3. Classifying order types...\")\n",
    "\n",
    "def classify_order_type(row):\n",
    "    \"\"\"Classify order types based on Auftrags-art and Tilde\"\"\"\n",
    "    auftragsart = str(row.get('Auftrags-art', '')).lower()\n",
    "    tilde = str(row.get('Tilde.Auftrag', '')).lower()\n",
    "    \n",
    "    if 'leergut' in auftragsart:\n",
    "        return 'Leergut'\n",
    "    elif tilde == 'ja':\n",
    "        return 'Pickup/Multi-leg'\n",
    "    elif 'retoure' in auftragsart or 'abholung' in auftragsart:\n",
    "        return 'Retoure/Abholung'\n",
    "    else:\n",
    "        return 'Delivery'\n",
    "\n",
    "df_featured['order_type'] = df_featured.apply(classify_order_type, axis=1)\n",
    "print(f\"   ✓ Order types:\")\n",
    "print(df_featured['order_type'].value_counts())\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Betriebszentralen Mapping\n",
    "# ==============================================================================\n",
    "print(\"\\n4. Mapping Betriebszentralen (14 dispatch centers)...\")\n",
    "\n",
    "try:\n",
    "    betriebszentralen_df = pd.read_csv('../data/raw/TRAVECO_Betriebszentralen.csv')\n",
    "    \n",
    "    # Ensure types match\n",
    "    df_featured['Nummer.Auftraggeber'] = pd.to_numeric(\n",
    "        df_featured['Nummer.Auftraggeber'],\n",
    "        errors='coerce'\n",
    "    ).astype('Int64')\n",
    "    \n",
    "    betriebszentralen_df['Nummer.Auftraggeber'] = pd.to_numeric(\n",
    "        betriebszentralen_df['Nummer.Auftraggeber'],\n",
    "        errors='coerce'\n",
    "    ).astype('Int64')\n",
    "    \n",
    "    # Create mapping with only needed columns\n",
    "    bz_mapping = betriebszentralen_df[['Nummer.Auftraggeber', 'Name1']].copy()\n",
    "    bz_mapping = bz_mapping.rename(columns={'Name1': 'betriebszentrale_name'})\n",
    "    \n",
    "    # Merge\n",
    "    df_featured = df_featured.merge(\n",
    "        bz_mapping,\n",
    "        on='Nummer.Auftraggeber',\n",
    "        how='left',\n",
    "        suffixes=('', '_bz')  # Add suffix to avoid conflicts\n",
    "    )\n",
    "    \n",
    "    # Check if merge worked\n",
    "    if 'betriebszentrale_name' in df_featured.columns:\n",
    "        # Fill missing\n",
    "        df_featured['betriebszentrale_name'] = df_featured['betriebszentrale_name'].fillna('Unknown')\n",
    "        \n",
    "        print(f\"   ✓ Betriebszentralen mapped successfully\")\n",
    "        print(f\"   ✓ Distribution:\")\n",
    "        bz_counts = df_featured['betriebszentrale_name'].value_counts()\n",
    "        for bz, count in bz_counts.head(15).items():\n",
    "            print(f\"      {bz}: {count:,}\")\n",
    "    else:\n",
    "        raise ValueError(\"betriebszentrale_name column not created after merge\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️  Could not load Betriebszentralen mapping: {e}\")\n",
    "    print(f\"      Creating 'Unknown' placeholder...\")\n",
    "    df_featured['betriebszentrale_name'] = 'Unknown'\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Sparten Mapping\n",
    "# ==============================================================================\n",
    "print(\"\\n5. Mapping Sparten (customer divisions)...\")\n",
    "\n",
    "try:\n",
    "    # Try loading Sparten from each year\n",
    "    sparten_files = [\n",
    "        '../data/raw/2024/2024 Sparten.xlsx',\n",
    "        '../data/raw/2023/2023 Sparten.xlsx',\n",
    "        '../data/raw/2022/2022 Sparten.xlsx'\n",
    "    ]\n",
    "    \n",
    "    sparten_df = None\n",
    "    for f in sparten_files:\n",
    "        if os.path.exists(f):\n",
    "            sparten_df = pd.read_excel(f)\n",
    "            print(f\"   ✓ Loaded Sparten from {os.path.basename(f)}\")\n",
    "            break\n",
    "    \n",
    "    if sparten_df is not None:\n",
    "        # Assuming first column is customer number, second is Sparte\n",
    "        customer_col = sparten_df.columns[0]\n",
    "        sparte_col = sparten_df.columns[1]\n",
    "        \n",
    "        # Convert to Int64 for matching\n",
    "        sparten_df[customer_col] = pd.to_numeric(\n",
    "            sparten_df[customer_col],\n",
    "            errors='coerce'\n",
    "        ).astype('Int64')\n",
    "        \n",
    "        if 'RKdNr' in df_featured.columns:\n",
    "            df_featured['RKdNr_numeric'] = pd.to_numeric(\n",
    "                df_featured['RKdNr'].astype(str).str.replace('-', '').str.strip(),\n",
    "                errors='coerce'\n",
    "            ).astype('Int64')\n",
    "            \n",
    "            # Create clean mapping\n",
    "            sparten_mapping = sparten_df[[customer_col, sparte_col]].copy()\n",
    "            sparten_mapping = sparten_mapping.rename(columns={sparte_col: 'sparte'})\n",
    "            \n",
    "            # Merge\n",
    "            df_featured = df_featured.merge(\n",
    "                sparten_mapping,\n",
    "                left_on='RKdNr_numeric',\n",
    "                right_on=customer_col,\n",
    "                how='left',\n",
    "                suffixes=('', '_sparten')  # Add suffix to avoid conflicts\n",
    "            )\n",
    "            \n",
    "            # Drop the merge key if it's different from our key\n",
    "            if customer_col in df_featured.columns and customer_col != 'RKdNr_numeric':\n",
    "                df_featured = df_featured.drop(columns=[customer_col])\n",
    "            \n",
    "            # Check if merge worked\n",
    "            if 'sparte' in df_featured.columns:\n",
    "                df_featured['sparte'] = df_featured['sparte'].fillna('Keine Sparte')\n",
    "                \n",
    "                print(f\"   ✓ Sparten mapped successfully\")\n",
    "                print(f\"   ✓ Top 10 Sparten:\")\n",
    "                sparten_counts = df_featured['sparte'].value_counts()\n",
    "                for sparte, count in sparten_counts.head(10).items():\n",
    "                    print(f\"      {sparte}: {count:,}\")\n",
    "            else:\n",
    "                raise ValueError(\"sparte column not created after merge\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  RKdNr column not found\")\n",
    "            df_featured['sparte'] = 'Unknown'\n",
    "    else:\n",
    "        print(f\"   ⚠️  No Sparten file found\")\n",
    "        df_featured['sparte'] = 'Unknown'\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️  Could not map Sparten: {e}\")\n",
    "    print(f\"      Creating 'Unknown' placeholder...\")\n",
    "    df_featured['sparte'] = 'Unknown'\n",
    "\n",
    "# ==============================================================================\n",
    "# Summary\n",
    "# ==============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Feature Engineering Complete!\")\n",
    "print(f\"  Final columns: {len(df_featured.columns)}\")\n",
    "print(f\"  Final records: {len(df_featured):,}\")\n",
    "print(f\"  Memory usage: {df_featured.memory_usage(deep=True).sum() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Data Quality Validation\n",
    "\n",
    "Final checks before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Data Quality Validation\n",
      "================================================================================\n",
      "\n",
      "1. Time Coverage:\n",
      "   Start: 2022-01-01 00:00:00\n",
      "   End: 2024-12-31 00:00:00\n",
      "   Total months: 36\n",
      "\n",
      "2. Duplicate Check:\n",
      "   ⚠️  Order ID column not found\n",
      "\n",
      "3. Feature Completeness:\n",
      "   year: 0 missing (0.00%)\n",
      "   month: 0 missing (0.00%)\n",
      "   quarter: 0 missing (0.00%)\n",
      "   carrier_type: 0 missing (0.00%)\n",
      "   order_type: 0 missing (0.00%)\n",
      "   betriebszentrale_name: 0 missing (0.00%)\n",
      "\n",
      "4. Summary Statistics:\n",
      "   Total orders: 4,940,044\n",
      "   Avg orders/month: 137223\n",
      "   Years covered: [np.int64(2022), np.int64(2023), np.int64(2024)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Data Quality Validation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Time coverage\n",
    "print(\"\\n1. Time Coverage:\")\n",
    "print(f\"   Start: {df_featured['Datum.Tour'].min()}\")\n",
    "print(f\"   End: {df_featured['Datum.Tour'].max()}\")\n",
    "print(f\"   Total months: {df_featured['year_month'].nunique()}\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\n2. Duplicate Check:\")\n",
    "if 'Auftragsschein-Nr.' in df_featured.columns:\n",
    "    dupes = df_featured.duplicated(subset=['Auftragsschein-Nr.'], keep=False).sum()\n",
    "    print(f\"   Duplicate order IDs: {dupes:,}\")\n",
    "else:\n",
    "    print(\"   ⚠️  Order ID column not found\")\n",
    "\n",
    "# Feature completeness\n",
    "print(\"\\n3. Feature Completeness:\")\n",
    "required_features = ['year', 'month', 'quarter', 'carrier_type', 'order_type', \n",
    "                     'betriebszentrale_name']\n",
    "for feat in required_features:\n",
    "    if feat in df_featured.columns:\n",
    "        missing = int(df_featured[feat].isna().sum())  # Convert to int!\n",
    "        pct = (missing / len(df_featured)) * 100\n",
    "        print(f\"   {feat}: {missing:,} missing ({pct:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"   ❌ {feat}: NOT FOUND\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n4. Summary Statistics:\")\n",
    "print(f\"   Total orders: {len(df_featured):,}\")\n",
    "print(f\"   Avg orders/month: {len(df_featured) / df_featured['year_month'].nunique():.0f}\")\n",
    "print(f\"   Years covered: {sorted(df_featured['source_year'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Save Consolidated Dataset\n",
    "\n",
    "Save to processed folder in compressed format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING CONSOLIDATED DATASET...\n",
      "================================================================================\n",
      "\n",
      "Saving to ../data/processed/historic_orders_2022_2024.pkl...\n",
      "✓ Saved Pickle: 3424.0 MB\n",
      "\n",
      "Saving compressed CSV to ../data/processed/historic_orders_2022_2024.csv.gz...\n",
      "✓ Saved CSV.gz: 421.4 MB\n",
      "\n",
      "Attempting Parquet save (optional)...\n",
      "⚠️  Parquet save skipped: Duplicate column names found: ['NummerKomplett.Auftrag', 'Nummer.Hauptauftrag', 'Nummer.Auftrag', 'Datum.Tour', 'Nummer.Tour', 'Tour Bezeichnung', 'Nummer.Auftraggeber', 'Id.Dispostelle', 'AuNr (formatiert)', 'AuNr (Original)', 'Auftrags-art', 'RKdNr', 'RKdArt', 'RKdName', 'RKdOrt', 'Nummer.Versender', 'Name.Versender', 'Versender Name 2', 'Strasse.Versender', 'Land.Versender', 'Ort.Versender', 'PLZ.Versender', 'Nummer.Empfänger', 'Name.Empfänger', 'Empfänger Name2', 'Land.Empfänger', 'Strasse.Empfänger', 'PLZ.Empfänger', 'Ort.Empfänger', 'Nummer.Beladestelle', 'Name.Beladestelle', 'Beladestelle Name 2', 'Land.Beladestelle', 'Strasse.Beladestelle', 'PLZ.Beladestelle', 'Ort.Beladestelle', 'Nummer.Entladestelle', 'Name.Entladestelle', 'Entladestelle Name 2', 'Land.Entladestelle', 'Strasse.Entladestelle', 'PLZ.Entladestelle', 'Ort.Entladestelle', '∑ Einheiten', '∑ Gewicht', '∑ Frachtpfl. Gewicht', 'Lieferart 2.0', '∑ Einnahmen', '∑ Ausgaben Gesamt', '∑ Ausgaben Spediteur', 'Ausgaben andere', '∑ Erfolg', 'Nummer.Fahrzeug', 'Kennzeichen.Fahrzeug', 'Nummer.Spedition', 'Name.Spedition', 'Beschränkungen', 'Vertreter.Auftrag', 'Gebühr.Auftrag', 'Bezeichnung.Sortiment', 'Ramseier Gruppen', 'Liefergebiet.Auftrag', 'Nummer.Artikel', 'Name1.Artikel', 'Text1.Auftrag', 'Text2.Auftrag', 'Text3.Auftrag', 'Frachtkosten', 'Herkunft.Auftrag', 'Versandart.Auftrag', 'hinterlegter Fixpreis', 'Kennzeichen Kontraktauftrag', 'Kontraktauftrag Nummer', 'Nummerisch2.Auftrag', 'Nummerisch3.Auftrag', 'Nummerisch4.Auftrag', 'SBB Paletten.Auftrag', 'RC', 'Transportmittel3.Auftrag', 'Transportmittel4.Auftrag', 'Transportmittel5.Auftrag', 'Transportmittel6.Auftrag', 'Transportmittel7.Auftrag', 'Transportmittel8.Auftrag', 'Transportmittel9.Auftrag', 'Transportmittel10.Auftrag', 'Anlage Datum', 'Planstatus.Auftrag', 'Rückerfasst.Auftrag', 'Erledigungstatus.Auftrag', 'Freigabe Abrechnung AU', 'Freigabe Abrechnung Tour', 'Belegnummer Gutschrift', 'Belegnummer Rechnung', 'Gutschrift erstellt', 'Rechnung erstellt', 'Staufaktor.Auftrag', 'TourTyp.Tour', 'Distanz_BE.Auftrag', 'Distanz_VE.Auftrag', 'System_id.Auftrag', 'Fakturiert', 'Tilde.Auftrag', 'source_file', 'source_year', 'year_month', 'year', 'month', 'quarter', 'week', 'day_of_year', 'weekday', 'is_weekend', 'carrier_type', 'order_type', 'betriebszentrale_name', 'RKdNr_numeric', 'sparte', 'betriebszentrale_name_bz', 'Kunden-Nr._sparten', 'sparte_sparten', 'betriebszentrale_name_bz', 'sparte_sparten']\n",
      "   (Pickle and CSV.gz versions are sufficient)\n",
      "\n",
      "================================================================================\n",
      "CONSOLIDATION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Dataset ready for time series forecasting:\n",
      "  • Records: 4,940,044\n",
      "  • Time range: 2022-01-01 00:00:00 to 2024-12-31 00:00:00\n",
      "  • Months: 36\n",
      "  • Features: 123\n",
      "  • Betriebszentralen: 12\n",
      "  • Carrier types: 3\n",
      "  • Order types: 4\n",
      "\n",
      "Files saved:\n",
      "  ✓ Pickle:  ../data/processed/historic_orders_2022_2024.pkl (FAST loading - recommended)\n",
      "  ✓ CSV.gz:  ../data/processed/historic_orders_2022_2024.csv.gz (Backup/portable)\n",
      "  ✓ Parquet: ../data/processed/historic_orders_2022_2024.parquet (Optional)\n",
      "\n",
      "Next step: Run notebook 08 for time series aggregation\n",
      "  Load command: df = pd.read_pickle('../data/processed/historic_orders_2022_2024.pkl')\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Section 7: Save Consolidated Dataset\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING CONSOLIDATED DATASET...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Save as Pickle (RECOMMENDED - handles all data types)\n",
    "# ==============================================================================\n",
    "output_pickle = output_dir / 'historic_orders_2022_2024.pkl'\n",
    "print(f\"\\nSaving to {output_pickle}...\")\n",
    "df_featured.to_pickle(output_pickle)\n",
    "file_size_mb = os.path.getsize(output_pickle) / 1024**2\n",
    "print(f\"✓ Saved Pickle: {file_size_mb:.1f} MB\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Save as CSV.gz (backup - slower but portable)\n",
    "# ==============================================================================\n",
    "output_csv = output_dir / 'historic_orders_2022_2024.csv.gz'\n",
    "print(f\"\\nSaving compressed CSV to {output_csv}...\")\n",
    "df_featured.to_csv(output_csv, index=False, compression='gzip')\n",
    "file_size_mb = os.path.getsize(output_csv) / 1024**2\n",
    "print(f\"✓ Saved CSV.gz: {file_size_mb:.1f} MB\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Try Parquet (optional - will skip if fails)\n",
    "# ==============================================================================\n",
    "print(f\"\\nAttempting Parquet save (optional)...\")\n",
    "try:\n",
    "  # Convert ALL object columns to string for Parquet compatibility\n",
    "  object_cols = df_featured.select_dtypes(include=['object']).columns\n",
    "  df_parquet = df_featured.copy()\n",
    "\n",
    "  for col in object_cols:\n",
    "      df_parquet[col] = df_parquet[col].astype(str)\n",
    "\n",
    "  output_parquet = output_dir / 'historic_orders_2022_2024.parquet'\n",
    "  df_parquet.to_parquet(output_parquet, index=False, compression='snappy')\n",
    "  file_size_mb = os.path.getsize(output_parquet) / 1024**2\n",
    "  print(f\"✓ Saved Parquet: {file_size_mb:.1f} MB\")\n",
    "except Exception as e:\n",
    "  print(f\"⚠️  Parquet save skipped: {e}\")\n",
    "  print(\"   (Pickle and CSV.gz versions are sufficient)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Summary\n",
    "# ==============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"CONSOLIDATION COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nDataset ready for time series forecasting:\")\n",
    "print(f\"  • Records: {len(df_featured):,}\")\n",
    "print(f\"  • Time range: {df_featured['Datum.Tour'].min()} to {df_featured['Datum.Tour'].max()}\")\n",
    "print(f\"  • Months: {df_featured['year'].nunique() * 12 if 'year' in df_featured.columns else 36}\")\n",
    "print(f\"  • Features: {len(df_featured.columns)}\")\n",
    "print(f\"  • Betriebszentralen: {df_featured['betriebszentrale_name'].nunique()}\")\n",
    "print(f\"  • Carrier types: {df_featured['carrier_type'].nunique()}\")\n",
    "print(f\"  • Order types: {df_featured['order_type'].nunique()}\")\n",
    "\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"  ✓ Pickle:  {output_pickle} (FAST loading - recommended)\")\n",
    "print(f\"  ✓ CSV.gz:  {output_csv} (Backup/portable)\")\n",
    "if 'output_parquet' in locals() and output_parquet.exists():\n",
    "  print(f\"  ✓ Parquet: {output_parquet} (Optional)\")\n",
    "\n",
    "print(f\"\\nNext step: Run notebook 08 for time series aggregation\")\n",
    "print(f\"  Load command: df = pd.read_pickle('{output_pickle}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Quick Preview\n",
    "\n",
    "Display sample records to verify data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTING RELOAD...\n",
      "================================================================================\n",
      "\n",
      "Reloading from ../data/processed/historic_orders_2022_2024.pkl...\n",
      "✓ Loaded 4,940,044 records in 6.9 seconds!\n",
      "  Columns: 123\n",
      "  Memory: 30.20 GB\n",
      "\n",
      "================================================================================\n",
      "Sample Records (first 5):\n",
      "================================================================================\n",
      "  Datum.Tour  year  month betriebszentrale_name        order_type carrier_type\n",
      "0 2022-01-01  2022      1          BZ Landquart  Pickup/Multi-leg     external\n",
      "1 2022-01-01  2022      1          BZ Landquart  Pickup/Multi-leg     external\n",
      "2 2022-01-01  2022      1          BZ Landquart  Pickup/Multi-leg     external\n",
      "3 2022-01-01  2022      1          BZ Landquart  Pickup/Multi-leg     external\n",
      "4 2022-01-01  2022      1          BZ Landquart  Pickup/Multi-leg     external\n",
      "\n",
      "✅ NOTEBOOK 07 COMPLETE!\n",
      "You can now proceed to Notebook 08 for time series aggregation.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Section 8: Quick Preview & Reload Test\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING RELOAD...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Reload from Pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "pickle_path = Path('../data/processed/historic_orders_2022_2024.pkl')\n",
    "print(f\"\\nReloading from {pickle_path}...\")\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "df_reloaded = pd.read_pickle(pickle_path)\n",
    "load_time = time.time() - start\n",
    "\n",
    "print(f\"✓ Loaded {len(df_reloaded):,} records in {load_time:.1f} seconds!\")\n",
    "print(f\"  Columns: {len(df_reloaded.columns)}\")\n",
    "print(f\"  Memory: {df_reloaded.memory_usage(deep=True).sum() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Sample records\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample Records (first 5):\")\n",
    "print(\"=\"*80)\n",
    "display_cols = ['Datum.Tour', 'year', 'month', 'betriebszentrale_name',\n",
    "              'order_type', 'carrier_type']\n",
    "available_cols = [col for col in display_cols if col in df_reloaded.columns]\n",
    "print(df_reloaded[available_cols].head())\n",
    "\n",
    "print(\"\\n✅ NOTEBOOK 07 COMPLETE!\")\n",
    "print(\"You can now proceed to Notebook 08 for time series aggregation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
